[0;32m[INFO][2021-11-30 11:07:00.748][pplnn.cc:800] [0mppl.nn version: ce3ab9267048492f2473897d36adefa630427d8d-dirty
==29138== NVPROF is profiling process 29138, command: ./pplnn-build/tools/pplnn --use-cuda --onnx-model /mnt/hpc/share/xusi/QuantRes18Splitter/Conv_23/model.onnx --inputs /mnt/hpc/share/xusi/QuantRes18Splitter/LayerOutput/140_decorated_by_act_quant.bin --quantization /mnt/hpc/share/xusi/QuantRes18Splitter/re
[0;32m[INFO][2021-11-30 11:07:01.232][pplnn.cc:243] [0m***** register CudaEngine *****
[0;32m[INFO][2021-11-30 11:07:01.233][engine_graph_partitioner.cc:104] [0mtotal partition(s) of graph[Conv_23]: 1.
[0;31m[ERROR][2021-11-30 11:07:01.234][gene_kernel.cc:657] [0m1 1
#define TILE_N_PER_CTA       32
#define TILE_M_PER_CTA       64

#define TILE_N_PER_WARP      8
#define TILE_M_PER_WARP      64

#define TILE_K_PER_CTA       256
#define TILE_K_PER_SET       64
#define TILE_K_PER_WARP      64

#define INTER_SET_REDUCE_RATIO  ((TILE_K_PER_CTA) / (TILE_K_PER_SET))

#define REDUCE(_R)            REDUCE_INT_3x4(_R)

#define READ_sRv4(_Rv4, _sm_base_v4, _sRv4_read)        READ_sRv4_SIZE4(_Rv4, _sm_base_v4, _sRv4_read)

#define KERNEL_NAME nv2spkConv_hmma8816_nhwc_f3_b64x32_w64x8_k256_s64_buf1
#define USE_1BUF

#define ENABLE_FUSE 1

#define uint int

#define uint32_t int

#define int16_t int

#define int8_t int

#define MAX_LUT_SIZE 128

#define MAX_SPLITK_SIZE 8

struct lut_t{ int idx[MAX_LUT_SIZE]; };

// Licensed to the Apache Software Foundation (ASF) under one
// or more contributor license agreements.  See the NOTICE file
// distributed with this work for additional information
// regarding copyright ownership.  The ASF licenses this file
// to you under the Apache License, Version 2.0 (the
// "License"); you may not use this file except in compliance
// with the License.  You may obtain a copy of the License at
//
//   http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing,
// software distributed under the License is distributed on an
// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
// KIND, either express or implied.  See the License for the
// specific language governing permissions and limitations
// under the License.

////////////////////////////////////////
// kernel list macros
////////////////////////////////////////

#define SPK_KPARAM_LIST \
        int4* dA,                                                 \
        int4* dB,                                                 \
        int4* dC,                                                 \
        int kloop_num,                                            \
        struct lut_t in_lut,          int in_lut_size,            \
        struct lut_t flt_lut,         int flt_lut_size,           \
        int in_hw,                    int out_hw,                 \
        int flt_hw,                   int splitk,                 \
        int in_height,                int in_width,               \
        int in_num,                   int num_grp,                \
        int num_chl_per_grp,          int num_chl_per_grp_pad,    \
        int flt_height,               int flt_width,              \
        int num_flt_per_grp,          int num_flt_per_grp_pad,    \
        int out_height,               int out_width,              \
        int stride_height,            int stride_width,           \
        int pad_height,               int pad_width,              \
        int hole_height,              int hole_width,             \
        int has_bias,                 int* bias                   \
	float in_scale,               void *d_flt_scale,          \
        float out_scale

#define TOTAL_KPARAM_LIST \
        int4* dA,                                                 \
        int4* dB,                                                 \
        int4* dC,                                                 \
        int kloop_num,                                            \
        struct lut_t in_lut,          int in_lut_size,            \
        struct lut_t flt_lut,         int flt_lut_size,           \
        int in_hw,                    int out_hw,                 \
        int flt_hw,                   int splitk,                 \
        int in_height,                int in_width,               \
        int in_num,                   int num_grp,                \
        int num_chl_per_grp,          int num_chl_per_grp_pad,    \
        int flt_height,               int flt_width,              \
        int num_flt_per_grp,          int num_flt_per_grp_pad,    \
        int out_height,               int out_width,              \
        int stride_height,            int stride_width,           \
        int pad_height,               int pad_width,              \
        int hole_height,              int hole_width,             \
        int  has_bias,                const int4* bias,           \
	float in_scale,               void *d_flt_scale,          \
        float out_scale,              float pre_scale,            \
        int  has_relu,                const float clip_min,     \
	    bool has_clip,                const float clip_max,     \
        int  has_prelu,               const void* prelu,          \
        bool has_elt,                 const int4* pre_data,       \
        int  has_elt_relu,            const float elt_clip_min, \
	    bool has_elt_clip,            const float elt_clip_max, \
        int has_elt_prelu,            const void* elt_prelu,      \
        const float leaky,           const float elt_leaky,     \
        bool has_concat,              int concat_offset_v16,       \
        int concat_stride_v16

////////////////////////////////////////
// align functions
////////////////////////////////////////

#define Align(x, y)   (((x) + (y) - 1) / (y) * (y))
#define DivUp(x, y)   (((x) + (y) - 1) / (y))

#define Min(x, y)     (((x) < (y)) ? (x) : (y))
#define Max(x, y)     (((x) > (y)) ? (x) : (y))

////////////////////////////////////////
// boundary check
////////////////////////////////////////

#define WidthInRange(_w)     ( (_w < in_width)  && (_w >= 0) )
#define HeightInRange(_h)    ( (_h < in_height) && (_h >= 0) )

////////////////////////////////////////
// constant cta size macros
////////////////////////////////////////

#define _16CHAR_TO_INT4_        16
#define _4CHAR_TO_INT_          4
#define _4INT_TO_INT4_          4
#define _2INT_TO_INT2_          2

#define _2HALF_TO_INT_          2
#define _2INT2_TO_INT4_         2

#define _C1_                    1
#define _C2_                    2
#define _C4_                    4
#define _C8_                    8
#define _C16_                   16
#define _C32_                   32

#define _1INT_                  1
#define _2INT_                  2
#define _4INT_                  4
#define _8INT_                  8

#define _1INT4_                 1
#define _2INT4_                 2
#define _4INT4_                 4
#define _8INT4_                 8

#define _1INT8_                 1
#define _2INT8_                 2
#define _4INT8_                 4
#define _8INT8_                 8

#define _1HALF_                 1
#define _2HALF_                 2
#define _4HALF_                 4
#define _8HALF_                 8

#define _1HALF2_                1
#define _2HALF2_                2
#define _4HALF2_                4
#define _8HALF2_                8

#define _1MMA_                  1
#define _2MMA_                  2
#define _4MMA_                  4
#define _8MMA_                  8

#define _HALF_ZERO_             0.0
#define _ZERO_                  0.0f


#define _INT_TO_BYTE_           4
#define _INT_TO_2HALF_          2
#define _INT2_TO_2HALF2_        2
#define _INT2_TO_2INT_          2

#define _INT4_TO_INT4_          1
#define _INT4_TO_2INT2_         2
#define _INT4_TO_4INT_          4
#define _INT4_TO_4HALF2_        4
#define _INT4_TO_8HALF_         8

#define SMEM_ROW_V8_SIZE        4
#define SMEM_ROW_V4_SIZE        8
#define SMEM_ROW_V2_SIZE        16
#define SMEM_ROW_V1_SIZE        32
#define SMEM_ROW_BYTE_SIZE      128
#define SMEM_ROW_BIT_SIZE       1024

////////////////////////////////////////
// mma size macros
////////////////////////////////////////

#define TILE_M_PER_MMA          8
#define TILE_K_PER_MMA          8
#define TILE_N_PER_MMA          8
#define TILE_M_PER_SUB_MMA      8

#define MMA_SIZE_X_IN_THD       4
#define MMA_SIZE_Y_IN_THD       8

////////////////////////////////////////
// thread / warp / cta size macros
////////////////////////////////////////

#define WARP_SIZE_IN_THD        32
#define WARP_SIZE_IN_BITS       5

#define WARP_SIZE_X_IN_THD      4
#define WARP_SIZE_Y_IN_THD      8

#define SET_SIZE_X_IN_WARP      ((TILE_N_PER_CTA) / (TILE_N_PER_WARP))
#define SET_SIZE_Y_IN_WARP      ((TILE_M_PER_CTA) / (TILE_M_PER_WARP))

#define SET_SIZE_IN_WARP        ((SET_SIZE_X_IN_WARP) * (SET_SIZE_Y_IN_WARP))
#define SET_SIZE_IN_THD         ((SET_SIZE_IN_WARP)   * (WARP_SIZE_IN_THD))

#define CTA_SIZE_IN_WARP        ((SET_SIZE_IN_WARP)   * (INTER_SET_REDUCE_RATIO))
#define CTA_SIZE_IN_THD         ((CTA_SIZE_IN_WARP)   * (WARP_SIZE_IN_THD))

#define WARP_SIZE_IN_THD_HALF   (WARP_SIZE_IN_THD / 2)
#define WARP_SIZE_IN_THD_QTR    (WARP_SIZE_IN_THD / 4)
////////////////////////////////////////
// tiling size macros
////////////////////////////////////////

#define TILE_M_PER_THD          ((TILE_M_PER_WARP) / (WARP_SIZE_Y_IN_THD))
#define TILE_N_PER_THD          ((TILE_N_PER_WARP) / (WARP_SIZE_X_IN_THD))

/////////////////////
// tile m

#define TILE_M_V1_PER_CTA       ((TILE_M_PER_CTA)  / 1)
#define TILE_M_V2_PER_CTA       ((TILE_M_PER_CTA)  / 2)
#define TILE_M_V4_PER_CTA       ((TILE_M_PER_CTA)  / 4)
#define TILE_M_V8_PER_CTA       ((TILE_M_PER_CTA)  / 8)

#define TILE_M_V1_PER_WARP      ((TILE_M_PER_WARP) / 1)
#define TILE_M_V2_PER_WARP      ((TILE_M_PER_WARP) / 2)
#define TILE_M_V4_PER_WARP      ((TILE_M_PER_WARP) / 4)
#define TILE_M_V8_PER_WARP      ((TILE_M_PER_WARP) / 8)

#define TILE_M_V1_PER_THD       ((TILE_M_PER_THD)  / 1)
#define TILE_M_V2_PER_THD       ((TILE_M_PER_THD)  / 2)
#define TILE_M_V4_PER_THD       ((TILE_M_PER_THD)  / 4)
#define TILE_M_V8_PER_THD       ((TILE_M_PER_THD)  / 8)

#define TILE_M_V1_PER_MMA       ((TILE_M_PER_MMA)  / 1)
#define TILE_M_V2_PER_MMA       ((TILE_M_PER_MMA)  / 2)
#define TILE_M_V4_PER_MMA       ((TILE_M_PER_MMA)  / 4)
#define TILE_M_V8_PER_MMA       ((TILE_M_PER_MMA)  / 8)

/////////////////////
// tile k

#define TILE_K_V1_PER_CTA       ((TILE_K_PER_CTA)  / 1)
#define TILE_K_V2_PER_CTA       ((TILE_K_PER_CTA)  / 2)
#define TILE_K_V4_PER_CTA       ((TILE_K_PER_CTA)  / 4)
#define TILE_K_V8_PER_CTA       ((TILE_K_PER_CTA)  / 8)
#define TILE_K_V16_PER_CTA      ((TILE_K_PER_CTA)  / 16)

#define TILE_K_V1_PER_SET       ((TILE_K_PER_SET)  / 1)
#define TILE_K_V2_PER_SET       ((TILE_K_PER_SET)  / 2)
#define TILE_K_V4_PER_SET       ((TILE_K_PER_SET)  / 4)
#define TILE_K_V8_PER_SET       ((TILE_K_PER_SET)  / 8)
#define TILE_K_V16_PER_SET      ((TILE_K_PER_SET)  / 16)

#define TILE_K_V1_PER_WARP      ((TILE_K_PER_WARP) / 1)
#define TILE_K_V2_PER_WARP      ((TILE_K_PER_WARP) / 2)
#define TILE_K_V4_PER_WARP      ((TILE_K_PER_WARP) / 4)
#define TILE_K_V8_PER_WARP      ((TILE_K_PER_WARP) / 8)

#define TILE_K_V1_PER_MMA       ((TILE_K_PER_MMA) / 1)
#define TILE_K_V2_PER_MMA       ((TILE_K_PER_MMA) / 2)
#define TILE_K_V4_PER_MMA       ((TILE_K_PER_MMA) / 4)
#define TILE_K_V8_PER_MMA       ((TILE_K_PER_MMA) / 8)

/////////////////////
// tile n

#define TILE_N_V1_PER_CTA       ((TILE_N_PER_CTA)  / 1)
#define TILE_N_V2_PER_CTA       ((TILE_N_PER_CTA)  / 2)
#define TILE_N_V4_PER_CTA       ((TILE_N_PER_CTA)  / 4)
#define TILE_N_V8_PER_CTA       ((TILE_N_PER_CTA)  / 8)

#define TILE_N_V1_PER_WARP      ((TILE_N_PER_WARP) / 1)
#define TILE_N_V2_PER_WARP      ((TILE_N_PER_WARP) / 2)
#define TILE_N_V4_PER_WARP      ((TILE_N_PER_WARP) / 4)
#define TILE_N_V8_PER_WARP      ((TILE_N_PER_WARP) / 8)

#define TILE_N_V1_PER_THD       ((TILE_N_PER_THD)  / 1)
#define TILE_N_V2_PER_THD       ((TILE_N_PER_THD)  / 2)
#define TILE_N_V4_PER_THD       ((TILE_N_PER_THD)  / 4)
#define TILE_N_V8_PER_THD       ((TILE_N_PER_THD)  / 8)

#define TILE_N_V1_PER_MMA       ((TILE_N_PER_MMA)  / 1)
#define TILE_N_V2_PER_MMA       ((TILE_N_PER_MMA)  / 2)
#define TILE_N_V4_PER_MMA       ((TILE_N_PER_MMA)  / 4)
#define TILE_N_V8_PER_MMA       ((TILE_N_PER_MMA)  / 8)

////////////////////////////////////////
// shared memory size macros
////////////////////////////////////////

//FIXME: the final is TILE_N_V16_PER_CTA
#define OUTPUT_STEPS            ((TILE_M_V1_PER_CTA) * (TILE_N_V4_PER_CTA) / CTA_SIZE_IN_THD)

#if OUTPUT_STEPS < 1
#undef  OUTPUT_STEPS
#define OUTPUT_STEPS  1
#endif

//#define N_ROWS_PER_SMEM_ROW     (SMEM_ROW_V4_SIZE / TILE_N_V8_PER_CTA)
//#define K_ROWS_PER_SMEM_ROW     (SMEM_ROW_V4_SIZE / TILE_K_V8_PER_CTA)
#define N_ROWS_PER_SMEM_ROW     (SMEM_ROW_V4_SIZE / TILE_N_V4_PER_CTA)
#define K_ROWS_PER_SMEM_ROW     (SMEM_ROW_V4_SIZE / TILE_K_V16_PER_CTA)

#if N_ROWS_PER_SMEM_ROW < 1
#undef  N_ROWS_PER_SMEM_ROW
#define N_ROWS_PER_SMEM_ROW 1
#endif

#if K_ROWS_PER_SMEM_ROW < 1
#undef  K_ROWS_PER_SMEM_ROW
#define K_ROWS_PER_SMEM_ROW 1
#endif

#define OUTPUT_SIZE_X_IN_THD    (TILE_N_V4_PER_CTA)
#define OUTPUT_SIZE_Y_IN_THD    ((CTA_SIZE_IN_THD) / (OUTPUT_SIZE_X_IN_THD))

////////////////////////////////////////
// k group macros
////////////////////////////////////////

#define SWITCH_BUFFER(_buf, _size, _base) \
        { \
            _buf = ((_buf - _base) ^ _size) + _base; \
        }

#define FWD_KGROUP_ODD(_sUv1_read) \
        { \
            _sUv1_read = _sUv1_read ^ 0x4; \
        }

#define FWD_KGROUP_EVEN(_sUv1_read) \
        { \
            _sUv1_read = _sUv1_read ^ 0xc; \
        }

#define FWD_KGROUP_STEP1(_sUv1_read)     FWD_KGROUP_ODD(_sUv1_read)
#define FWD_KGROUP_STEP2(_sUv1_read)     FWD_KGROUP_EVEN(_sUv1_read)
#define FWD_KGROUP_STEP3(_sUv1_read)     FWD_KGROUP_ODD(_sUv1_read)
#define FWD_KGROUP_STEP4(_sUv1_read)     FWD_KGROUP_EVEN(_sUv1_read)

////////////////////////////////////////
// main loop macros
////////////////////////////////////////

#define   C_ITEMS_PER_THD       ((TILE_M_PER_CTA) * (TILE_N_PER_CTA) / (SET_SIZE_IN_THD))
#define  HC_ITEMS_PER_THD       ((TILE_M_PER_CTA) * (TILE_N_PER_CTA) / (SET_SIZE_IN_THD))
//#define Cv4_ITEMS_PER_THD       ((TILE_M_PER_CTA) * (TILE_N_PER_CTA) / (SET_SIZE_IN_THD * _4CHAR_TO_INT_ * _4INT_TO_INT4_))
#define Cv4_ITEMS_PER_THD       ((TILE_M_PER_CTA) * (TILE_N_PER_CTA) / (SET_SIZE_IN_THD * _4INT_TO_INT4_))

#if Cv4_ITEMS_PER_THD < 1
#undef Cv4_ITEMS_PER_THD
#define Cv4_ITEMS_PER_THD 1
#endif

////////////////////////////////////////
// load A and B from device memory macros
////////////////////////////////////////

#define REG_dAv4_SIZE           ( ((TILE_M_PER_CTA) * (TILE_K_PER_CTA)) / ((_4CHAR_TO_INT_) * (_4INT_TO_INT4_) * (CTA_SIZE_IN_THD)) )
#define REG_dBv4_SIZE           ( ((TILE_N_PER_CTA) * (TILE_K_PER_CTA)) / ((_4CHAR_TO_INT_) * (_4INT_TO_INT4_) * (CTA_SIZE_IN_THD)) )

#if REG_dAv4_SIZE < 1
#undef  REG_dAv4_SIZE
#define REG_dAv4_SIZE 1
#endif

#if REG_dBv4_SIZE < 1
#undef  REG_dBv4_SIZE
#define REG_dBv4_SIZE 1
#endif

#define READ_dAv4_STEPS         (REG_dAv4_SIZE)
#define READ_dBv4_STEPS         (REG_dBv4_SIZE)

////////////////////////////////////////
// shared memory size macros
////////////////////////////////////////

#define SM_A_SIZE               ((TILE_M_PER_CTA) * (TILE_K_PER_CTA) / (_4CHAR_TO_INT_))
#define SM_B_SIZE               ((TILE_K_PER_CTA) * (TILE_N_PER_CTA) / (_4CHAR_TO_INT_))
#define SM_C_SIZE               ((TILE_M_PER_CTA) * (TILE_N_PER_CTA) / (_1INT_))

#define SM_A_1BUF               (SM_A_SIZE)
#define SM_B_1BUF               (SM_B_SIZE)
#define SM_C_1BUF               (SM_C_SIZE)

#define SM_A_2BUF               ((SM_A_SIZE) * 2)
#define SM_B_2BUF               ((SM_B_SIZE) * 2)
#define SM_C_2BUF               ((SM_C_SIZE) * 2)

#define SM_A_V1_1BUF            (SM_A_1BUF)
#define SM_B_V1_1BUF            (SM_B_1BUF)
#define SM_C_V1_1BUF            (SM_C_1BUF)

#define SM_A_V2_1BUF            ((SM_A_1BUF) / (_2INT_TO_INT2_))
#define SM_B_V2_1BUF            ((SM_B_1BUF) / (_2INT_TO_INT2_))
#define SM_C_V2_1BUF            ((SM_C_1BUF) / (_2INT_TO_INT2_))

#define SM_A_V4_1BUF            ((SM_A_1BUF) / (_4INT_TO_INT4_))
#define SM_B_V4_1BUF            ((SM_B_1BUF) / (_4INT_TO_INT4_))
#define SM_C_V4_1BUF            ((SM_C_1BUF) / (_4INT_TO_INT4_))

#define SM_A_V1_2BUF            ((SM_A_V1_1BUF) * 2)
#define SM_B_V1_2BUF            ((SM_B_V1_1BUF) * 2)
#define SM_C_V1_2BUF            ((SM_C_V1_1BUF) * 2)

#define SM_A_V2_2BUF            ((SM_A_V2_1BUF) * 2)
#define SM_B_V2_2BUF            ((SM_B_V2_1BUF) * 2)
#define SM_C_V2_2BUF            ((SM_C_V2_1BUF) * 2)

#define SM_A_V4_2BUF            ((SM_A_V4_1BUF) * 2)
#define SM_B_V4_2BUF            ((SM_B_V4_1BUF) * 2)
#define SM_C_V4_2BUF            ((SM_C_V4_1BUF) * 2)

#define SM_BASE_V4_1BUF         Max((SM_A_V4_1BUF + SM_B_V4_1BUF), (SM_C_V4_1BUF * INTER_SET_REDUCE_RATIO))
#define SM_BASE_V4_2BUF         Max((SM_A_V4_2BUF + SM_B_V4_2BUF), (SM_C_V4_1BUF * INTER_SET_REDUCE_RATIO))

#define CVT_SM_PTR(smp_base_v1, sm_base_v1) \
    asm("{ .reg .u64 smp_base_v1; cvta.to.shared.u64 smp_base_v1, %1; cvt.u32.u64 %0, smp_base_v1; }\n" \
            : "=r"(smp_base_v1) : "l"(sm_base_v1));

#define FWD_LUT(_lut_id) \
        { \
            _lut_id = (_lut_id == flt_hw) ? 1 : _lut_id + 1; \
        }
#define MAX(x, y)  ( (x) >= (y) ? (x) : (y) )
#define MIN(x, y)  ( (x) <= (y) ? (x) : (y) )
#define MMAs_PER_REDUCE_ROW     MIN( (TILE_N_V1_PER_CTA/TILE_N_V1_PER_MMA), SMEM_ROW_V8_SIZE )
#define TILE_N_IN_MMA_PER_WARP  ( TILE_N_V1_PER_WARP/TILE_N_V1_PER_MMA ) 
#define SWIZZLE_GROUP ( MAX( 1, (4/(CTA_SIZE_IN_THD/TILE_N_V4_PER_CTA)) ) )

////////////////////////////////////////
// bit size macros
////////////////////////////////////////

#if SET_SIZE_X_IN_WARP == 1
#define SET_SIZE_X_IN_BITS      0
#elif SET_SIZE_X_IN_WARP == 2
#define SET_SIZE_X_IN_BITS      1
#elif SET_SIZE_X_IN_WARP == 4
#define SET_SIZE_X_IN_BITS      2
#elif SET_SIZE_X_IN_WARP == 8
#define SET_SIZE_X_IN_BITS      3
#endif

#if MMA_SIZE_X_IN_THD == 1
#define MMA_SIZE_X_IN_BITS      0
#elif MMA_SIZE_X_IN_THD == 2
#define MMA_SIZE_X_IN_BITS      1
#elif MMA_SIZE_X_IN_THD == 4
#define MMA_SIZE_X_IN_BITS      2
#elif MMA_SIZE_X_IN_THD == 8
#define MMA_SIZE_X_IN_BITS      3
#endif

#if SET_SIZE_IN_WARP == 1
#define SET_SIZE_IN_BITS        5
#elif SET_SIZE_IN_WARP == 2
#define SET_SIZE_IN_BITS        6
#elif SET_SIZE_IN_WARP == 4
#define SET_SIZE_IN_BITS        7
#elif SET_SIZE_IN_WARP == 8
#define SET_SIZE_IN_BITS        8
#endif


// Licensed to the Apache Software Foundation (ASF) under one
// or more contributor license agreements.  See the NOTICE file
// distributed with this work for additional information
// regarding copyright ownership.  The ASF licenses this file
// to you under the Apache License, Version 2.0 (the
// "License"); you may not use this file except in compliance
// with the License.  You may obtain a copy of the License at
//
//   http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing,
// software distributed under the License is distributed on an
// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
// KIND, either express or implied.  See the License for the
// specific language governing permissions and limitations
// under the License.

#define SET_BOUND_FLT3(_in_hw_mask, _in_n_id, _in_h_id, _in_w_id) \
        { \
            if(_in_n_id < in_num) \
            { \
                _in_hw_mask = 0xffffffff; \
                if(_in_h_id < 0 || _in_h_id >= in_height) _in_hw_mask = _in_hw_mask & 0xfffffff8; \
                if(_in_w_id < 0 || _in_w_id >= in_width)  _in_hw_mask = _in_hw_mask & 0xffffffb6; \
                \
                _in_h_id += hole_height; \
                _in_w_id += hole_width; \
                \
                if(_in_h_id < 0 || _in_h_id >= in_height) _in_hw_mask = _in_hw_mask & 0xffffffc7; \
                if(_in_w_id < 0 || _in_w_id >= in_width)  _in_hw_mask = _in_hw_mask & 0xffffff6d; \
                \
                _in_h_id += hole_height; \
                _in_w_id += hole_width; \
                \
                if(_in_h_id < 0 || _in_h_id >= in_height)  _in_hw_mask = _in_hw_mask & 0xfffffe3f; \
                if(_in_w_id < 0 || _in_w_id >= in_width)   _in_hw_mask = _in_hw_mask & 0xfffffedb; \
            } else { \
                _in_hw_mask = 0x0; \
            } \
        }

#define FWD_FLT3(_flt_hw_id, _flt_hw_bid, _flt_c_v16_id, _flt_c_v16_valid) \
        { \
            if(_flt_hw_id == 8) \
            { \
                _flt_hw_id = 0; \
                _flt_c_v16_id += TILE_K_V16_PER_CTA; \
                \
                _flt_c_v16_valid = _flt_c_v16_id < flt_c_v16_end; \
            } else { \
                _flt_hw_id = _flt_hw_id + 1; \
            } \
            \
            _flt_hw_bid = (0x1 << _flt_hw_id); \
        }

#define FWD_FLT(_flt_hw_id, _flt_hw_bid, _flt_c_v16_id, _flt_c_v16_valid)    FWD_FLT3(_flt_hw_id, _flt_hw_bid, _flt_c_v16_id, _flt_c_v16_valid)


// Licensed to the Apache Software Foundation (ASF) under one
// or more contributor license agreements.  See the NOTICE file
// distributed with this work for additional information
// regarding copyright ownership.  The ASF licenses this file
// to you under the Apache License, Version 2.0 (the
// "License"); you may not use this file except in compliance
// with the License.  You may obtain a copy of the License at
//
//   http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing,
// software distributed under the License is distributed on an
// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
// KIND, either express or implied.  See the License for the
// specific language governing permissions and limitations
// under the License.

////////////////////////////////////////
// ldsm macros
////////////////////////////////////////

#define LDSM_ROW_X1_OPCODE \
        "ldmatrix.sync.aligned.x1.m8n8.shared.b16 {%0}, [%1];\n"

#define LDSM_ROW_X2_OPCODE \
        "ldmatrix.sync.aligned.x2.m8n8.shared.b16 {%0,%1}, [%2];\n"

#define LDSM_ROW_X4_OPCODE \
        "ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%0,%1,%2,%3}, [%4];\n"

#define LDSM_ROW_X1_INST(_x0, _addr) \
        asm volatile(LDSM_ROW_X1_OPCODE:   "=r"(_x0)   : "r"(_addr));

#define LDSM_ROW_X2_INST(_x0, _x1, _addr) \
        asm volatile(LDSM_ROW_X2_OPCODE:   "=r"(_x0),   "=r"(_x1): "r"(_addr));

#define LDSM_ROW_X4_INST(_x0, _x1, _x2, _x3, _addr) \
        asm volatile(LDSM_ROW_X4_OPCODE:   "=r"(_x0),   "=r"(_x1),  "=r"(_x2),   "=r"(_x3): "r"(_addr));

#define LDSM_COL_X1_OPCODE \
        "ldmatrix.sync.aligned.x1.trans.m8n8.shared.b16 {%0}, [%1];\n"

#define LDSM_COL_X2_OPCODE \
        "ldmatrix.sync.aligned.x2.trans.m8n8.shared.b16 {%0,%1}, [%2];\n"

#define LDSM_COL_X4_OPCODE \
        "ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%0,%1,%2,%3}, [%4];\n"

#define LDSM_COL_X1_INST(_x0, _addr) \
        asm volatile(LDSM_COL_X1_OPCODE:   "=r"(_x0)   : "r"(_addr));

#define LDSM_COL_X2_INST(_x0, _x1, _addr) \
        asm volatile(LDSM_COL_X2_OPCODE:   "=r"(_x0),   "=r"(_x1): "r"(_addr));

#define LDSM_COL_X4_INST(_x0, _x1, _x2, _x3, _addr) \
        asm volatile(LDSM_COL_X4_OPCODE:   "=r"(_x0),   "=r"(_x1),  "=r"(_x2),   "=r"(_x3): "r"(_addr));


// Licensed to the Apache Software Foundation (ASF) under one
// or more contributor license agreements.  See the NOTICE file
// distributed with this work for additional information
// regarding copyright ownership.  The ASF licenses this file
// to you under the Apache License, Version 2.0 (the
// "License"); you may not use this file except in compliance
// with the License.  You may obtain a copy of the License at
//
//   http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing,
// software distributed under the License is distributed on an
// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
// KIND, either express or implied.  See the License for the
// specific language governing permissions and limitations
// under the License.

////////////////////////////////////////
// load dB macros
////////////////////////////////////////

#define LOAD_dBv4_SIZE_16TH(_regB, _dB, _dBv4_off, _flt_c_v8_valid, _flt_n_valid) \
        { \
            _dBv4_off[0] += flt_lut.idx[lut_id]; \
            \
            if(tid < (CTA_SIZE_IN_THD / 16))  \
                _regB[0] = (_flt_n_valid[0] && _flt_c_v8_valid) ? _dB[ _dBv4_off[0] ] : ZEROv4;\
        }

#define LOAD_dBv4_SIZE_8TH(_regB, _dB, _dBv4_off, _flt_c_v8_valid, _flt_n_valid) \
        { \
            _dBv4_off[0] += flt_lut.idx[lut_id]; \
            \
            if(tid < (CTA_SIZE_IN_THD / 8))  \
                _regB[0] = (_flt_n_valid[0] && _flt_c_v8_valid) ? _dB[ _dBv4_off[0] ] : ZEROv4;\
        }

#define LOAD_dBv4_SIZE_QTR(_regB, _dB, _dBv4_off, _flt_c_v8_valid, _flt_n_valid) \
        { \
            _dBv4_off[0] += flt_lut.idx[lut_id]; \
            \
            if(tid < (CTA_SIZE_IN_THD / 4))  \
                _regB[0] = (_flt_n_valid[0] && _flt_c_v8_valid) ? _dB[ _dBv4_off[0] ] : ZEROv4;\
        }

#define LOAD_dBv4_SIZE_HALF(_regB, _dB, _dBv4_off, _flt_c_v8_valid, _flt_n_valid) \
        { \
            _dBv4_off[0] += flt_lut.idx[lut_id]; \
            \
            if(tid < (CTA_SIZE_IN_THD / 2))  \
                _regB[0] = (_flt_n_valid[0] && _flt_c_v8_valid) ? _dB[ _dBv4_off[0] ] : ZEROv4;\
        }

#define LOAD_dBv4_SIZE1(_regB, _dB, _dBv4_off, _flt_c_v8_valid, _flt_n_valid) \
        { \
            _dBv4_off[0] += flt_lut.idx[lut_id]; \
            \
            _regB[0] = (_flt_n_valid[0] && _flt_c_v8_valid) ? _dB[ _dBv4_off[0] ] : ZEROv4;\
        }

#define LOAD_dBv4_SIZE2(_regB, _dB, _dBv4_off, _flt_c_v8_valid, _flt_n_valid) \
        { \
            _dBv4_off[0] += flt_lut.idx[lut_id]; \
            _dBv4_off[1] += flt_lut.idx[lut_id]; \
            \
            _regB[0] = (_flt_n_valid[0] && _flt_c_v8_valid) ? _dB[ _dBv4_off[0] ] : ZEROv4;\
            _regB[1] = (_flt_n_valid[1] && _flt_c_v8_valid) ? _dB[ _dBv4_off[1] ] : ZEROv4;\
        }

#define LOAD_dBv4_SIZE4(_regB, _dB, _dBv4_off, _flt_c_v8_valid, _flt_n_valid) \
        { \
            _dBv4_off[0] += flt_lut.idx[lut_id]; \
            _dBv4_off[1] += flt_lut.idx[lut_id]; \
            _dBv4_off[2] += flt_lut.idx[lut_id]; \
            _dBv4_off[3] += flt_lut.idx[lut_id]; \
            \
            _regB[0] = (_flt_n_valid[0] && _flt_c_v8_valid) ? _dB[ _dBv4_off[0] ] : ZEROv4;\
            _regB[1] = (_flt_n_valid[1] && _flt_c_v8_valid) ? _dB[ _dBv4_off[1] ] : ZEROv4;\
            _regB[2] = (_flt_n_valid[2] && _flt_c_v8_valid) ? _dB[ _dBv4_off[2] ] : ZEROv4;\
            _regB[3] = (_flt_n_valid[3] && _flt_c_v8_valid) ? _dB[ _dBv4_off[3] ] : ZEROv4;\
        }

#define LOAD_dBv4_SIZE8(_regB, _dB, _dBv4_off, _flt_c_v8_valid, _flt_n_valid) \
        { \
            _dBv4_off[0] += flt_lut.idx[lut_id]; \
            _dBv4_off[1] += flt_lut.idx[lut_id]; \
            _dBv4_off[2] += flt_lut.idx[lut_id]; \
            _dBv4_off[3] += flt_lut.idx[lut_id]; \
            _dBv4_off[4] += flt_lut.idx[lut_id]; \
            _dBv4_off[5] += flt_lut.idx[lut_id]; \
            _dBv4_off[6] += flt_lut.idx[lut_id]; \
            _dBv4_off[7] += flt_lut.idx[lut_id]; \
            \
            _regB[0] = (_flt_n_valid[0] && _flt_c_v8_valid) ? _dB[ _dBv4_off[0] ] : ZEROv4;\
            _regB[1] = (_flt_n_valid[1] && _flt_c_v8_valid) ? _dB[ _dBv4_off[1] ] : ZEROv4;\
            _regB[2] = (_flt_n_valid[2] && _flt_c_v8_valid) ? _dB[ _dBv4_off[2] ] : ZEROv4;\
            _regB[3] = (_flt_n_valid[3] && _flt_c_v8_valid) ? _dB[ _dBv4_off[3] ] : ZEROv4;\
            _regB[4] = (_flt_n_valid[4] && _flt_c_v8_valid) ? _dB[ _dBv4_off[4] ] : ZEROv4;\
            _regB[5] = (_flt_n_valid[5] && _flt_c_v8_valid) ? _dB[ _dBv4_off[5] ] : ZEROv4;\
            _regB[6] = (_flt_n_valid[6] && _flt_c_v8_valid) ? _dB[ _dBv4_off[6] ] : ZEROv4;\
            _regB[7] = (_flt_n_valid[7] && _flt_c_v8_valid) ? _dB[ _dBv4_off[7] ] : ZEROv4;\
        }

#define SET_dBv4_BOUND(_step_id, _dBv4_off, _flt_n_valid) \
        { \
            int _flt_n_id  =  cta_idx *  TILE_N_PER_CTA + \
                            _step_id  * (TILE_N_PER_CTA / READ_dBv4_STEPS) + \
                             ldg_idy; \
            \
            _flt_n_valid  =  _flt_n_id < num_flt_per_grp; \
            \
            _dBv4_off  =   grp_id   * flt_hw * num_chl_per_grp_pad_v16 * num_flt_per_grp + \
                          _flt_n_id * flt_hw * num_chl_per_grp_pad_v16 + \
                           flt_c_v16_id; \
        }

////////////////////////////////////////
// load dA macros
////////////////////////////////////////

#define SET_dAv4_BOUND(_step_id, _dAv4_off, _in_hw_mask) \
        { \
            int _out_nhw_id =  cta_idy *  TILE_M_PER_CTA + \
                              _step_id * (TILE_M_PER_CTA / READ_dAv4_STEPS) + \
                              ldg_idy; \
            \
            int _out_w_id =  (_out_nhw_id % out_width); \
            int _out_h_id =  (_out_nhw_id / out_width) % out_height; \
            \
            int _in_n_id  =   _out_nhw_id / out_hw; \
            int _in_h_id  =     _out_h_id * stride_height; \
            int _in_w_id  =     _out_w_id * stride_width; \
            \
            _dAv4_off  =  (_in_n_id  * in_hw + _in_h_id  * in_width + _in_w_id) * num_chl_per_grp_pad_v16 * num_grp + \
                           grp_id   * num_chl_per_grp_pad_v16 + \
                           flt_c_v16_id; \
            \
            _in_h_id =  _in_h_id - pad_height; \
            _in_w_id =  _in_w_id - pad_width;  \
            \
            SET_BOUND_FLT3(_in_hw_mask, _in_n_id, _in_h_id, _in_w_id); \
        }

#define LOAD_dAv4_SIZE_16TH(_regA, _dA, _dAv4_off, _in_c_v8_valid, _flt_hw_bid) \
        { \
            _dAv4_off[0] += in_lut.idx[lut_id]; \
            \
            if(tid < (CTA_SIZE_IN_THD / 16))  \
                _regA[0] = ((_flt_hw_bid & in_hw_mask[0]) && _in_c_v8_valid) ? _dA[ _dAv4_off[0] ] : ZEROv4;\
        }

#define LOAD_dAv4_SIZE_8TH(_regA, _dA, _dAv4_off, _in_c_v8_valid, _flt_hw_bid) \
        { \
            _dAv4_off[0] += in_lut.idx[lut_id]; \
            \
            if(tid < (CTA_SIZE_IN_THD / 8))  \
                _regA[0] = ((_flt_hw_bid & in_hw_mask[0]) && _in_c_v8_valid) ? _dA[ _dAv4_off[0] ] : ZEROv4;\
        }

#define LOAD_dAv4_SIZE_QTR(_regA, _dA, _dAv4_off, _in_c_v8_valid, _flt_hw_bid) \
        { \
            _dAv4_off[0] += in_lut.idx[lut_id]; \
            \
            if(tid < (CTA_SIZE_IN_THD / 4))  \
                _regA[0] = ((_flt_hw_bid & in_hw_mask[0]) && _in_c_v8_valid) ? _dA[ _dAv4_off[0] ] : ZEROv4;\
        }

#define LOAD_dAv4_SIZE_HALF(_regA, _dA, _dAv4_off, _in_c_v8_valid, _flt_hw_bid) \
        { \
            _dAv4_off[0] += in_lut.idx[lut_id]; \
            \
            if(tid < (CTA_SIZE_IN_THD / 2))  \
                _regA[0] = ((_flt_hw_bid & in_hw_mask[0]) && _in_c_v8_valid) ? _dA[ _dAv4_off[0] ] : ZEROv4;\
        }

#define LOAD_dAv4_SIZE1(_regA, _dA, _dAv4_off, _in_c_v8_valid, _flt_hw_bid) \
        { \
            _dAv4_off[0] += in_lut.idx[lut_id]; \
            \
            _regA[0] = ((_flt_hw_bid & in_hw_mask[0]) && _in_c_v8_valid) ? _dA[ _dAv4_off[0] ] : ZEROv4;\
        }

#define LOAD_dAv4_SIZE2(_regA, _dA, _dAv4_off, _in_c_v8_valid, _flt_hw_bid) \
        { \
            _dAv4_off[0] += in_lut.idx[lut_id]; \
            _dAv4_off[1] += in_lut.idx[lut_id]; \
            \
            _regA[0] = ((_flt_hw_bid & in_hw_mask[0]) && _in_c_v8_valid) ? _dA[ _dAv4_off[0] ] : ZEROv4;\
            _regA[1] = ((_flt_hw_bid & in_hw_mask[1]) && _in_c_v8_valid) ? _dA[ _dAv4_off[1] ] : ZEROv4;\
        }

#define LOAD_dAv4_SIZE4(_regA, _dA, _dAv4_off, _in_c_v8_valid, _flt_hw_bid) \
        { \
            _dAv4_off[0] += in_lut.idx[lut_id]; \
            _dAv4_off[1] += in_lut.idx[lut_id]; \
            _dAv4_off[2] += in_lut.idx[lut_id]; \
            _dAv4_off[3] += in_lut.idx[lut_id]; \
            \
            _regA[0] = ((_flt_hw_bid & in_hw_mask[0]) && _in_c_v8_valid) ? _dA[ _dAv4_off[0] ] : ZEROv4;\
            _regA[1] = ((_flt_hw_bid & in_hw_mask[1]) && _in_c_v8_valid) ? _dA[ _dAv4_off[1] ] : ZEROv4;\
            _regA[2] = ((_flt_hw_bid & in_hw_mask[2]) && _in_c_v8_valid) ? _dA[ _dAv4_off[2] ] : ZEROv4;\
            _regA[3] = ((_flt_hw_bid & in_hw_mask[3]) && _in_c_v8_valid) ? _dA[ _dAv4_off[3] ] : ZEROv4;\
        }

#define LOAD_dAv4_SIZE8(_regA, _dA, _dAv4_off, _in_c_v8_valid, _flt_hw_bid) \
        { \
            _dAv4_off[0] += in_lut.idx[lut_id]; \
            _dAv4_off[1] += in_lut.idx[lut_id]; \
            _dAv4_off[2] += in_lut.idx[lut_id]; \
            _dAv4_off[3] += in_lut.idx[lut_id]; \
            _dAv4_off[4] += in_lut.idx[lut_id]; \
            _dAv4_off[5] += in_lut.idx[lut_id]; \
            _dAv4_off[6] += in_lut.idx[lut_id]; \
            _dAv4_off[7] += in_lut.idx[lut_id]; \
            \
            _regA[0] = ((_flt_hw_bid & in_hw_mask[0]) && _in_c_v8_valid) ? _dA[ _dAv4_off[0] ] : ZEROv4;\
            _regA[1] = ((_flt_hw_bid & in_hw_mask[1]) && _in_c_v8_valid) ? _dA[ _dAv4_off[1] ] : ZEROv4;\
            _regA[2] = ((_flt_hw_bid & in_hw_mask[2]) && _in_c_v8_valid) ? _dA[ _dAv4_off[2] ] : ZEROv4;\
            _regA[3] = ((_flt_hw_bid & in_hw_mask[3]) && _in_c_v8_valid) ? _dA[ _dAv4_off[3] ] : ZEROv4;\
            _regA[4] = ((_flt_hw_bid & in_hw_mask[4]) && _in_c_v8_valid) ? _dA[ _dAv4_off[4] ] : ZEROv4;\
            _regA[5] = ((_flt_hw_bid & in_hw_mask[5]) && _in_c_v8_valid) ? _dA[ _dAv4_off[5] ] : ZEROv4;\
            _regA[6] = ((_flt_hw_bid & in_hw_mask[6]) && _in_c_v8_valid) ? _dA[ _dAv4_off[6] ] : ZEROv4;\
            _regA[7] = ((_flt_hw_bid & in_hw_mask[7]) && _in_c_v8_valid) ? _dA[ _dAv4_off[7] ] : ZEROv4;\
        }

#define LOAD_dAv4_SIZE16(_regA, _dA, _dAv4_off, _in_c_v8_valid, _flt_hw_bid) \
        { \
            _dAv4_off[0]  += in_lut.idx[lut_id]; \
            _dAv4_off[1]  += in_lut.idx[lut_id]; \
            _dAv4_off[2]  += in_lut.idx[lut_id]; \
            _dAv4_off[3]  += in_lut.idx[lut_id]; \
            _dAv4_off[4]  += in_lut.idx[lut_id]; \
            _dAv4_off[5]  += in_lut.idx[lut_id]; \
            _dAv4_off[6]  += in_lut.idx[lut_id]; \
            _dAv4_off[7]  += in_lut.idx[lut_id]; \
            _dAv4_off[8]  += in_lut.idx[lut_id]; \
            _dAv4_off[9]  += in_lut.idx[lut_id]; \
            _dAv4_off[10] += in_lut.idx[lut_id]; \
            _dAv4_off[11] += in_lut.idx[lut_id]; \
            _dAv4_off[12] += in_lut.idx[lut_id]; \
            _dAv4_off[13] += in_lut.idx[lut_id]; \
            _dAv4_off[14] += in_lut.idx[lut_id]; \
            _dAv4_off[15] += in_lut.idx[lut_id]; \
            \
            _regA[0]  = ((_flt_hw_bid & in_hw_mask[0])  && _in_c_v8_valid) ? _dA[ _dAv4_off[0]  ] : ZEROv4;\
            _regA[1]  = ((_flt_hw_bid & in_hw_mask[1])  && _in_c_v8_valid) ? _dA[ _dAv4_off[1]  ] : ZEROv4;\
            _regA[2]  = ((_flt_hw_bid & in_hw_mask[2])  && _in_c_v8_valid) ? _dA[ _dAv4_off[2]  ] : ZEROv4;\
            _regA[3]  = ((_flt_hw_bid & in_hw_mask[3])  && _in_c_v8_valid) ? _dA[ _dAv4_off[3]  ] : ZEROv4;\
            _regA[4]  = ((_flt_hw_bid & in_hw_mask[4])  && _in_c_v8_valid) ? _dA[ _dAv4_off[4]  ] : ZEROv4;\
            _regA[5]  = ((_flt_hw_bid & in_hw_mask[5])  && _in_c_v8_valid) ? _dA[ _dAv4_off[5]  ] : ZEROv4;\
            _regA[6]  = ((_flt_hw_bid & in_hw_mask[6])  && _in_c_v8_valid) ? _dA[ _dAv4_off[6]  ] : ZEROv4;\
            _regA[7]  = ((_flt_hw_bid & in_hw_mask[7])  && _in_c_v8_valid) ? _dA[ _dAv4_off[7]  ] : ZEROv4;\
            _regA[8]  = ((_flt_hw_bid & in_hw_mask[8])  && _in_c_v8_valid) ? _dA[ _dAv4_off[8]  ] : ZEROv4;\
            _regA[9]  = ((_flt_hw_bid & in_hw_mask[9])  && _in_c_v8_valid) ? _dA[ _dAv4_off[9]  ] : ZEROv4;\
            _regA[10] = ((_flt_hw_bid & in_hw_mask[10]) && _in_c_v8_valid) ? _dA[ _dAv4_off[10] ] : ZEROv4;\
            _regA[11] = ((_flt_hw_bid & in_hw_mask[11]) && _in_c_v8_valid) ? _dA[ _dAv4_off[11] ] : ZEROv4;\
            _regA[12] = ((_flt_hw_bid & in_hw_mask[12]) && _in_c_v8_valid) ? _dA[ _dAv4_off[12] ] : ZEROv4;\
            _regA[13] = ((_flt_hw_bid & in_hw_mask[13]) && _in_c_v8_valid) ? _dA[ _dAv4_off[13] ] : ZEROv4;\
            _regA[14] = ((_flt_hw_bid & in_hw_mask[14]) && _in_c_v8_valid) ? _dA[ _dAv4_off[14] ] : ZEROv4;\
            _regA[15] = ((_flt_hw_bid & in_hw_mask[15]) && _in_c_v8_valid) ? _dA[ _dAv4_off[15] ] : ZEROv4;\
        }



// Licensed to the Apache Software Foundation (ASF) under one
// or more contributor license agreements.  See the NOTICE file
// distributed with this work for additional information
// regarding copyright ownership.  The ASF licenses this file
// to you under the Apache License, Version 2.0 (the
// "License"); you may not use this file except in compliance
// with the License.  You may obtain a copy of the License at
//
//   http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing,
// software distributed under the License is distributed on an
// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
// KIND, either express or implied.  See the License for the
// specific language governing permissions and limitations
// under the License.

////////////////////////////////////////
// hmma macros
////////////////////////////////////////

// int8 input, int32 output
#define MMA_8816_INST_OPCODE \
        "mma.sync.aligned.m8n8k16.row.col.s32.s8.s8.s32 {%0,%1}, {%2}, {%3}, {%4,%5};\n"

#define MMA_INST_OPCODE \
        "mma.sync.aligned.m16n8k8.row.col.f16.f16.f16.f16 {%0,%1}, {%2,%3}, {%4}, {%5,%6};\n"
        
#define MMA_INST(_d0, _d1, _a, _b) \
        asm volatile(MMA_8816_INST_OPCODE:   "=r"(_d0),   "=r"(_d1): "r"(_a), "r"(_b),  "r"(_d0),   "r"(_d1));

#define MMA_INST_ASCEND1(_C, _C_off, _C_stride, _a0, _B) \
        { \
            MMA_INST(_C[_C_off],     _C[_C_off + 1], _a0, _B[0]); \
        }
        
#define MMA_INST_ASCEND2(_C, _C_off, _C_stride, _a0, _B) \
        { \
            MMA_INST(_C[_C_off],     _C[_C_off + 1], _a0, _B[0]); \
            MMA_INST(_C[_C_off + 2], _C[_C_off + 3], _a0, _B[1]); \
        }
        
#define MMA_INST_ASCEND4(_C, _C_off, _C_stride, _a0, _B) \
        { \
            MMA_INST(_C[_C_off],     _C[_C_off + 1], _a0, _B[0]); \
            MMA_INST(_C[_C_off + 2], _C[_C_off + 3], _a0, _B[1]); \
            MMA_INST(_C[_C_off + 4], _C[_C_off + 5], _a0, _B[2]); \
            MMA_INST(_C[_C_off + 6], _C[_C_off + 7], _a0, _B[3]); \
        }
        
#define MMA_INST_ASCEND8(_C, _C_off, _C_stride, _a0, _B) \
        { \
            MMA_INST(_C[_C_off],      _C[_C_off + 1],     _a0, _B[0]); \
            MMA_INST(_C[_C_off + 2],  _C[_C_off + 3],     _a0, _B[1]); \
            MMA_INST(_C[_C_off + 4],  _C[_C_off + 5],     _a0, _B[2]); \
            MMA_INST(_C[_C_off + 6],  _C[_C_off + 7],     _a0, _B[3]); \
            MMA_INST(_C[_C_off + 8],  _C[_C_off + 9],     _a0, _B[4]); \
            MMA_INST(_C[_C_off + 10], _C[_C_off + 11],    _a0, _B[5]); \
            MMA_INST(_C[_C_off + 12], _C[_C_off + 13],    _a0, _B[6]); \
            MMA_INST(_C[_C_off + 14], _C[_C_off + 15],    _a0, _B[7]); \
        }
        
       
#define MMA_INST_DESCEND1(_C, _C_off, _C_stride, _a0, _B) \
        { \
            MMA_INST(_C[_C_off - 1],     _C[_C_off],     _a0, _B[0]); \
        }

#define MMA_INST_DESCEND2(_C, _C_off, _C_stride, _a0, _B) \
        { \
            MMA_INST(_C[_C_off - 1],     _C[_C_off],     _a0, _B[1]); \
            MMA_INST(_C[_C_off - 3], _C[_C_off - 2],     _a0, _B[0]); \
        }

#define MMA_INST_DESCEND4(_C, _C_off, _C_stride, _a0, _B) \
        { \
            MMA_INST(_C[_C_off - 1],     _C[_C_off],     _a0, _B[3]); \
            MMA_INST(_C[_C_off - 3], _C[_C_off - 2],     _a0, _B[2]); \
            MMA_INST(_C[_C_off - 5], _C[_C_off - 4],     _a0, _B[1]); \
            MMA_INST(_C[_C_off - 7], _C[_C_off - 6],     _a0, _B[0]); \
        }

#define MMA_INST_DESCEND8(_C, _C_off, _C_stride, _a0, _B) \
        { \
            MMA_INST(_C[_C_off - 1],      _C[_C_off],  _a0, _B[7]); \
            MMA_INST(_C[_C_off - 3],  _C[_C_off - 2],  _a0, _B[6]); \
            MMA_INST(_C[_C_off - 5],  _C[_C_off - 4],  _a0, _B[5]); \
            MMA_INST(_C[_C_off - 7],  _C[_C_off - 6],  _a0, _B[4]); \
            MMA_INST(_C[_C_off - 9],  _C[_C_off - 8],  _a0, _B[3]); \
            MMA_INST(_C[_C_off - 11], _C[_C_off - 10],  _a0, _B[2]); \
            MMA_INST(_C[_C_off - 13], _C[_C_off - 12], _a0, _B[1]); \
            MMA_INST(_C[_C_off - 15], _C[_C_off - 14], _a0, _B[0]); \
        }

#define MMA_INST_1x1(_C, _A, _B) \
        { \
            MMA_INST_ASCEND1  (_C, 0,  TILE_N_V2_PER_THD, _A[0], _B); \
        }

#define MMA_INST_1x2(_C, _A, _B) \
        { \
            MMA_INST_ASCEND2  (_C, 0,  TILE_N_V2_PER_THD, _A[0], _B); \
        }

#define MMA_INST_1x4(_C, _A, _B) \
        { \
            MMA_INST_ASCEND4  (_C, 0,  TILE_N_V2_PER_THD, _A[0], _B); \
        }

#define MMA_INST_1x8(_C, _A, _B) \
        { \
            MMA_INST_ASCEND8  (_C, 0,  TILE_N_V2_PER_THD, _A[0], _B); \
        }

#define MMA_INST_2x1(_C, _A, _B) \
        { \
            MMA_INST_ASCEND1  (_C, 0,  TILE_N_V2_PER_THD, _A[0], _B); \
            MMA_INST_DESCEND1 (_C, 3,  TILE_N_V2_PER_THD, _A[1], _B); \
        }

#define MMA_INST_2x2(_C, _A, _B) \
        { \
            MMA_INST_ASCEND2  (_C, 0,  TILE_N_V2_PER_THD, _A[0], _B); \
            MMA_INST_DESCEND2 (_C, 7,  TILE_N_V2_PER_THD, _A[1], _B); \
        }

#define MMA_INST_2x4(_C, _A, _B) \
        { \
            MMA_INST_ASCEND4  (_C, 0,  TILE_N_V2_PER_THD, _A[0], _B); \
            MMA_INST_DESCEND4 (_C, 15, TILE_N_V2_PER_THD, _A[1], _B); \
        }

#define MMA_INST_2x8(_C, _A, _B) \
        { \
            MMA_INST_ASCEND8  (_C, 0,  TILE_N_V2_PER_THD, _A[0], _B); \
            MMA_INST_DESCEND8 (_C, 31, TILE_N_V2_PER_THD, _A[1], _B); \
        }

#define MMA_INST_4x1(_C, _A, _B) \
        { \
            MMA_INST_ASCEND1  (_C, 0,  TILE_N_V2_PER_THD, _A[0], _B); \
            MMA_INST_DESCEND1 (_C, 3,  TILE_N_V2_PER_THD, _A[1], _B); \
            \
            MMA_INST_ASCEND1  (_C, 4,  TILE_N_V2_PER_THD, _A[2], _B); \
            MMA_INST_DESCEND1 (_C, 7,  TILE_N_V2_PER_THD, _A[3], _B); \
        }

#define MMA_INST_4x2(_C, _A, _B) \
        { \
            MMA_INST_ASCEND2  (_C, 0,  TILE_N_V2_PER_THD, _A[0], _B); \
            MMA_INST_DESCEND2 (_C, 7,  TILE_N_V2_PER_THD, _A[1], _B); \
            \
            MMA_INST_ASCEND2  (_C, 8,  TILE_N_V2_PER_THD, _A[2], _B); \
            MMA_INST_DESCEND2 (_C, 15, TILE_N_V2_PER_THD, _A[3], _B); \
        }

#define MMA_INST_4x4(_C, _A, _B) \
        { \
            MMA_INST_ASCEND4  (_C, 0,  TILE_N_V2_PER_THD, _A[0], _B); \
            MMA_INST_DESCEND4 (_C, 15, TILE_N_V2_PER_THD, _A[1], _B); \
            \
            MMA_INST_ASCEND4  (_C, 16, TILE_N_V2_PER_THD, _A[2], _B); \
            MMA_INST_DESCEND4 (_C, 31, TILE_N_V2_PER_THD, _A[3], _B); \
        }

#define MMA_INST_4x8(_C, _A, _B) \
        { \
            MMA_INST_ASCEND8  (_C, 0,  TILE_N_V2_PER_THD, _A[0], _B); \
            MMA_INST_DESCEND8 (_C, 31, TILE_N_V2_PER_THD, _A[1], _B); \
            \
            MMA_INST_ASCEND8  (_C, 32, TILE_N_V2_PER_THD, _A[2], _B); \
            MMA_INST_DESCEND8 (_C, 63, TILE_N_V2_PER_THD, _A[3], _B); \
        }

#define MMA_INST_8x1(_C, _A, _B) \
        { \
            MMA_INST_ASCEND1  (_C, 0,  TILE_N_V2_PER_THD, _A[0],  _B); \
            MMA_INST_DESCEND1 (_C, 3,  TILE_N_V2_PER_THD, _A[1],  _B); \
            \
            MMA_INST_ASCEND1  (_C, 4,  TILE_N_V2_PER_THD, _A[2],  _B); \
            MMA_INST_DESCEND1 (_C, 7,  TILE_N_V2_PER_THD, _A[3],  _B); \
            \
            MMA_INST_ASCEND1  (_C, 8,  TILE_N_V2_PER_THD, _A[4],  _B); \
            MMA_INST_DESCEND1 (_C, 11, TILE_N_V2_PER_THD, _A[5],  _B); \
            \
            MMA_INST_ASCEND1  (_C, 12, TILE_N_V2_PER_THD, _A[6],  _B); \
            MMA_INST_DESCEND1 (_C, 15, TILE_N_V2_PER_THD, _A[7],  _B); \
        }

#define MMA_INST_8x2(_C, _A, _B) \
        { \
            MMA_INST_ASCEND2  (_C, 0,  TILE_N_V2_PER_THD, _A[0],  _B); \
            MMA_INST_DESCEND2 (_C, 7,  TILE_N_V2_PER_THD, _A[1],  _B); \
            \
            MMA_INST_ASCEND2  (_C, 8,  TILE_N_V2_PER_THD, _A[2],  _B); \
            MMA_INST_DESCEND2 (_C, 15, TILE_N_V2_PER_THD, _A[3],  _B); \
            \
            MMA_INST_ASCEND2  (_C, 16, TILE_N_V2_PER_THD, _A[4],  _B); \
            MMA_INST_DESCEND2 (_C, 23, TILE_N_V2_PER_THD, _A[5],  _B); \
            \
            MMA_INST_ASCEND2  (_C, 24, TILE_N_V2_PER_THD, _A[6],  _B); \
            MMA_INST_DESCEND2 (_C, 31, TILE_N_V2_PER_THD, _A[7],  _B); \
        }

#define MMA_INST_8x4(_C, _A, _B) \
        { \
            MMA_INST_ASCEND4  (_C, 0,  TILE_N_V2_PER_THD, _A[0],  _B); \
            MMA_INST_DESCEND4 (_C, 15, TILE_N_V2_PER_THD, _A[1],  _B); \
            \
            MMA_INST_ASCEND4  (_C, 16, TILE_N_V2_PER_THD, _A[2],  _B); \
            MMA_INST_DESCEND4 (_C, 31, TILE_N_V2_PER_THD, _A[3],  _B); \
            \
            MMA_INST_ASCEND4  (_C, 32, TILE_N_V2_PER_THD, _A[4],  _B); \
            MMA_INST_DESCEND4 (_C, 47, TILE_N_V2_PER_THD, _A[5],  _B); \
            \
            MMA_INST_ASCEND4  (_C, 48, TILE_N_V2_PER_THD, _A[6],  _B); \
            MMA_INST_DESCEND4 (_C, 63, TILE_N_V2_PER_THD, _A[7],  _B); \
        }

#define MMA_INST_8x8(_C, _A, _B) \
        { \
            MMA_INST_ASCEND8  (_C, 0,   TILE_N_V2_PER_THD, _A[0], _B); \
            MMA_INST_DESCEND8 (_C, 31,  TILE_N_V2_PER_THD, _A[1], _B); \
            \
            MMA_INST_ASCEND8  (_C, 32,  TILE_N_V2_PER_THD, _A[2], _B); \
            MMA_INST_DESCEND8 (_C, 63,  TILE_N_V2_PER_THD, _A[3], _B); \
	    \
            MMA_INST_ASCEND8  (_C, 64,  TILE_N_V2_PER_THD, _A[4], _B); \
            MMA_INST_DESCEND8 (_C, 95,  TILE_N_V2_PER_THD, _A[5], _B); \
            \
            MMA_INST_ASCEND8  (_C, 96,  TILE_N_V2_PER_THD, _A[6], _B); \
            MMA_INST_DESCEND8 (_C, 127, TILE_N_V2_PER_THD, _A[7], _B); \
        }

#define MMA_INST_16x1(_C, _A, _B) \
        { \
            MMA_INST_ASCEND1  (_C, 0,  TILE_N_V2_PER_THD, _A[0],  _B); \
            MMA_INST_DESCEND1 (_C, 3,  TILE_N_V2_PER_THD, _A[1],  _B); \
            \
            MMA_INST_ASCEND1  (_C, 4,  TILE_N_V2_PER_THD, _A[2],  _B); \
            MMA_INST_DESCEND1 (_C, 7,  TILE_N_V2_PER_THD, _A[3],  _B); \
            \
            MMA_INST_ASCEND1  (_C, 8,  TILE_N_V2_PER_THD, _A[4],  _B); \
            MMA_INST_DESCEND1 (_C, 11, TILE_N_V2_PER_THD, _A[5],  _B); \
            \
            MMA_INST_ASCEND1  (_C, 12, TILE_N_V2_PER_THD, _A[6],  _B); \
            MMA_INST_DESCEND1 (_C, 15, TILE_N_V2_PER_THD, _A[7],  _B); \
	    \
            MMA_INST_ASCEND1  (_C, 16, TILE_N_V2_PER_THD, _A[8],  _B); \
            MMA_INST_DESCEND1 (_C, 19, TILE_N_V2_PER_THD, _A[9],  _B); \
            \
            MMA_INST_ASCEND1  (_C, 20, TILE_N_V2_PER_THD, _A[10], _B); \
            MMA_INST_DESCEND1 (_C, 23, TILE_N_V2_PER_THD, _A[11], _B); \
            \
            MMA_INST_ASCEND1  (_C, 24, TILE_N_V2_PER_THD, _A[12], _B); \
            MMA_INST_DESCEND1 (_C, 27, TILE_N_V2_PER_THD, _A[13], _B); \
            \
            MMA_INST_ASCEND1  (_C, 28, TILE_N_V2_PER_THD, _A[14], _B); \
            MMA_INST_DESCEND1 (_C, 31, TILE_N_V2_PER_THD, _A[15], _B); \
        }

#define MMA_INST_16x2(_C, _A, _B) \
        { \
            MMA_INST_ASCEND2  (_C, 0,  TILE_N_V2_PER_THD, _A[0],  _B); \
            MMA_INST_DESCEND2 (_C, 7,  TILE_N_V2_PER_THD, _A[1],  _B); \
            \
            MMA_INST_ASCEND2  (_C, 8,  TILE_N_V2_PER_THD, _A[2],  _B); \
            MMA_INST_DESCEND2 (_C, 15, TILE_N_V2_PER_THD, _A[3],  _B); \
            \
            MMA_INST_ASCEND2  (_C, 16, TILE_N_V2_PER_THD, _A[4],  _B); \
            MMA_INST_DESCEND2 (_C, 23, TILE_N_V2_PER_THD, _A[5],  _B); \
            \
            MMA_INST_ASCEND2  (_C, 24, TILE_N_V2_PER_THD, _A[6],  _B); \
            MMA_INST_DESCEND2 (_C, 31, TILE_N_V2_PER_THD, _A[7],  _B); \
	    \
            MMA_INST_ASCEND2  (_C, 32, TILE_N_V2_PER_THD, _A[8],  _B); \
            MMA_INST_DESCEND2 (_C, 39, TILE_N_V2_PER_THD, _A[9],  _B); \
            \
            MMA_INST_ASCEND2  (_C, 40, TILE_N_V2_PER_THD, _A[10], _B); \
            MMA_INST_DESCEND2 (_C, 47, TILE_N_V2_PER_THD, _A[11], _B); \
            \
            MMA_INST_ASCEND2  (_C, 48, TILE_N_V2_PER_THD, _A[12], _B); \
            MMA_INST_DESCEND2 (_C, 55, TILE_N_V2_PER_THD, _A[13], _B); \
            \
            MMA_INST_ASCEND2  (_C, 56, TILE_N_V2_PER_THD, _A[14], _B); \
            MMA_INST_DESCEND2 (_C, 63, TILE_N_V2_PER_THD, _A[15], _B); \
        }

#define MMA_INST_16x4(_C, _A, _B) \
        { \
            MMA_INST_ASCEND4  (_C, 0,   TILE_N_V2_PER_THD, _A[0],  _B); \
            MMA_INST_DESCEND4 (_C, 15,  TILE_N_V2_PER_THD, _A[1],  _B); \
            \
            MMA_INST_ASCEND4  (_C, 16,  TILE_N_V2_PER_THD, _A[2],  _B); \
            MMA_INST_DESCEND4 (_C, 31,  TILE_N_V2_PER_THD, _A[3],  _B); \
            \
            MMA_INST_ASCEND4  (_C, 32,  TILE_N_V2_PER_THD, _A[4],  _B); \
            MMA_INST_DESCEND4 (_C, 47,  TILE_N_V2_PER_THD, _A[5],  _B); \
            \
            MMA_INST_ASCEND4  (_C, 48,  TILE_N_V2_PER_THD, _A[6],  _B); \
            MMA_INST_DESCEND4 (_C, 63,  TILE_N_V2_PER_THD, _A[7],  _B); \
	    \
            MMA_INST_ASCEND4  (_C, 64,  TILE_N_V2_PER_THD, _A[8],  _B); \
            MMA_INST_DESCEND4 (_C, 79,  TILE_N_V2_PER_THD, _A[9],  _B); \
            \
            MMA_INST_ASCEND4  (_C, 80,  TILE_N_V2_PER_THD, _A[10], _B); \
            MMA_INST_DESCEND4 (_C, 95,  TILE_N_V2_PER_THD, _A[11], _B); \
            \
            MMA_INST_ASCEND4  (_C, 96,  TILE_N_V2_PER_THD, _A[12], _B); \
            MMA_INST_DESCEND4 (_C, 111, TILE_N_V2_PER_THD, _A[13], _B); \
            \
            MMA_INST_ASCEND4  (_C, 112, TILE_N_V2_PER_THD, _A[14], _B); \
            MMA_INST_DESCEND4 (_C, 127, TILE_N_V2_PER_THD, _A[15], _B); \
        }



// Licensed to the Apache Software Foundation (ASF) under one
// or more contributor license agreements.  See the NOTICE file
// distributed with this work for additional information
// regarding copyright ownership.  The ASF licenses this file
// to you under the Apache License, Version 2.0 (the
// "License"); you may not use this file except in compliance
// with the License.  You may obtain a copy of the License at
//
//   http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing,
// software distributed under the License is distributed on an
// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
// KIND, either express or implied.  See the License for the
// specific language governing permissions and limitations
// under the License.

/////////////////////////////////////////////////////
// reduce half2 macros
/////////////////////////////////////////////////////

#define REDUCE_INT_SIZE4(_R, _R_off) \
        { \
            _R[0] = _R[0] + _R[_R_off]; \
            _R[1] = _R[1] + _R[_R_off + 1]; \
            _R[2] = _R[2] + _R[_R_off + 2]; \
            _R[3] = _R[3] + _R[_R_off + 3]; \
        }

#define REDUCE_INT_1x4(_R) \
        { \
            REDUCE_INT_SIZE4(_R, _4INT_); \
        }

#define REDUCE_INT_3x4(_R) \
        { \
            REDUCE_INT_SIZE4(_R, _4INT_); \
            REDUCE_INT_SIZE4(_R, _4INT_ * 2); \
            REDUCE_INT_SIZE4(_R, _4INT_ * 3); \
        }

/////////////////////////////////////////////////////
// read sRv4 macros
/////////////////////////////////////////////////////

#define READ_sRv4_SIZE1(_Rv4, _sm_base_v4, _sRv4_read) \
        { \
            if(dCv4_x_valid) \
            { \
                _Rv4[0] = _sm_base_v4[_sRv4_read]; \
            } \
            \
            _sRv4_read += CTA_SIZE_IN_THD * SWIZZLE_GROUP; \
        }

#define READ_sRv4_SIZE2(_Rv4, _sm_base_v4, _sRv4_read) \
        { \
            if(dCv4_x_valid) \
            { \
                _Rv4[0] = _sm_base_v4[_sRv4_read]; \
                _Rv4[1] = _sm_base_v4[_sRv4_read + TILE_M_V1_PER_CTA * TILE_N_V4_PER_CTA * 1]; \
            } \
            \
            _sRv4_read += CTA_SIZE_IN_THD * SWIZZLE_GROUP; \
        }

#define READ_sRv4_SIZE4(_Rv4, _sm_base_v4, _sRv4_read) \
        { \
            if(dCv4_x_valid) \
            { \
                _Rv4[0] = _sm_base_v4[_sRv4_read]; \
                _Rv4[1] = _sm_base_v4[_sRv4_read + TILE_M_V1_PER_CTA * TILE_N_V4_PER_CTA * 1]; \
                _Rv4[2] = _sm_base_v4[_sRv4_read + TILE_M_V1_PER_CTA * TILE_N_V4_PER_CTA * 2]; \
                _Rv4[3] = _sm_base_v4[_sRv4_read + TILE_M_V1_PER_CTA * TILE_N_V4_PER_CTA * 3]; \
            } \
            \
            _sRv4_read += CTA_SIZE_IN_THD * SWIZZLE_GROUP; \
        }


/////////////////////////////////////////////////////
// write sRv2 macros
/////////////////////////////////////////////////////

// actually is THD_IN_N_PER_MMA
#define WRITE_sRv2_SIZE1(_sm_base_v2, _sRv2_write, _i2C, _C_off) \
        { \
	    /*if(blockIdx.x+blockIdx.y+blockIdx.z==0)    printf("tid:%d\tv2_off:%d\tv2write:%d\trow_off:%d\n", tid, _sRv2_write + (smem_row_write_off ^ 0x0) * TILE_N_V2_PER_MMA, _sRv2_write, smem_row_write_off); */\
            _sm_base_v2[_sRv2_write + (smem_row_write_off ^ 0x0) * TILE_N_V2_PER_MMA] = _i2C[_C_off + 0]; \
        }

#define WRITE_sRv2_SIZE2(_sm_base_v2, _sRv2_write, _i2C, _C_off) \
        { \
            _sm_base_v2[_sRv2_write + (smem_row_write_off ^ 0x0) * TILE_N_V2_PER_MMA] = _i2C[_C_off + 0]; \
            _sm_base_v2[_sRv2_write + (smem_row_write_off ^ 0x1) * TILE_N_V2_PER_MMA] = _i2C[_C_off + 1]; \
        }

#define WRITE_sRv2_SIZE4(_sm_base_v2, _sRv2_write, _i2C, _C_off) \
        { \
            _sm_base_v2[_sRv2_write + (smem_row_write_off ^ 0x0) * TILE_N_V2_PER_MMA] = _i2C[_C_off + 0]; \
            _sm_base_v2[_sRv2_write + (smem_row_write_off ^ 0x1) * TILE_N_V2_PER_MMA] = _i2C[_C_off + 1]; \
            _sm_base_v2[_sRv2_write + (smem_row_write_off ^ 0x2) * TILE_N_V2_PER_MMA] = _i2C[_C_off + 2]; \
            _sm_base_v2[_sRv2_write + (smem_row_write_off ^ 0x3) * TILE_N_V2_PER_MMA] = _i2C[_C_off + 3]; \
        }

#define WRITE_sRv2_SIZE8(_sm_base_v2, _sRv2_write, _i2C, _C_off) \
        { \
            _sm_base_v2[_sRv2_write + (smem_row_write_off ^ 0x0) * TILE_N_V2_PER_MMA] = _i2C[_C_off + 0]; \
            _sm_base_v2[_sRv2_write + (smem_row_write_off ^ 0x1) * TILE_N_V2_PER_MMA] = _i2C[_C_off + 1]; \
            _sm_base_v2[_sRv2_write + (smem_row_write_off ^ 0x2) * TILE_N_V2_PER_MMA] = _i2C[_C_off + 2]; \
            _sm_base_v2[_sRv2_write + (smem_row_write_off ^ 0x3) * TILE_N_V2_PER_MMA] = _i2C[_C_off + 3]; \
            _sm_base_v2[_sRv2_write + (smem_row_write_off ^ 0x4) * TILE_N_V2_PER_MMA] = _i2C[_C_off + 4]; \
            _sm_base_v2[_sRv2_write + (smem_row_write_off ^ 0x5) * TILE_N_V2_PER_MMA] = _i2C[_C_off + 5]; \
            _sm_base_v2[_sRv2_write + (smem_row_write_off ^ 0x6) * TILE_N_V2_PER_MMA] = _i2C[_C_off + 6]; \
            _sm_base_v2[_sRv2_write + (smem_row_write_off ^ 0x7) * TILE_N_V2_PER_MMA] = _i2C[_C_off + 7]; \
        }

/////////////////////////
// tile_n_per_warp = 8
/////////////////////////

#define WRITE_sRv2_1x1(_sm_base_v2, _sRv2_write, _i2C) \
        { \
            WRITE_sRv2_SIZE1(_sm_base_v2, _sRv2_write, _i2C, 0); \
        }

#define WRITE_sRv2_2x1(_sm_base_v2, _sRv2_write, _i2C) \
        { \
            WRITE_sRv2_SIZE1(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 0, _i2C, _1MMA_ * 0); \
            WRITE_sRv2_SIZE1(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 1, _i2C, _1MMA_ * 1); \
        }

#define WRITE_sRv2_4x1(_sm_base_v2, _sRv2_write, _i2C) \
        { \
            WRITE_sRv2_SIZE1(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 0, _i2C, _1MMA_ * 0); \
            WRITE_sRv2_SIZE1(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 1, _i2C, _1MMA_ * 1); \
            WRITE_sRv2_SIZE1(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 2, _i2C, _1MMA_ * 2); \
            WRITE_sRv2_SIZE1(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 3, _i2C, _1MMA_ * 3); \
        }

#define WRITE_sRv2_8x1(_sm_base_v2, _sRv2_write, _i2C) \
        { \
            WRITE_sRv2_SIZE1(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 0, _i2C, _1MMA_ * 0); \
            WRITE_sRv2_SIZE1(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 1, _i2C, _1MMA_ * 1); \
            WRITE_sRv2_SIZE1(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 2, _i2C, _1MMA_ * 2); \
            WRITE_sRv2_SIZE1(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 3, _i2C, _1MMA_ * 3); \
            WRITE_sRv2_SIZE1(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 4, _i2C, _1MMA_ * 4); \
            WRITE_sRv2_SIZE1(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 5, _i2C, _1MMA_ * 5); \
            WRITE_sRv2_SIZE1(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 6, _i2C, _1MMA_ * 6); \
            WRITE_sRv2_SIZE1(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 7, _i2C, _1MMA_ * 7); \
        }

#define WRITE_sRv2_16x1(_sm_base_v2, _sRv2_write, _i2C) \
        { \
            WRITE_sRv2_SIZE1(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 0,  _i2C, _1MMA_ * 0); \
            WRITE_sRv2_SIZE1(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 1,  _i2C, _1MMA_ * 1); \
            WRITE_sRv2_SIZE1(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 2,  _i2C, _1MMA_ * 2); \
            WRITE_sRv2_SIZE1(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 3,  _i2C, _1MMA_ * 3); \
            WRITE_sRv2_SIZE1(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 4,  _i2C, _1MMA_ * 4); \
            WRITE_sRv2_SIZE1(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 5,  _i2C, _1MMA_ * 5); \
            WRITE_sRv2_SIZE1(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 6,  _i2C, _1MMA_ * 6); \
            WRITE_sRv2_SIZE1(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 7,  _i2C, _1MMA_ * 7); \
            WRITE_sRv2_SIZE1(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 8,  _i2C, _1MMA_ * 8); \
            WRITE_sRv2_SIZE1(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 9,  _i2C, _1MMA_ * 9); \
            WRITE_sRv2_SIZE1(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 10, _i2C, _1MMA_ * 10); \
            WRITE_sRv2_SIZE1(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 11, _i2C, _1MMA_ * 11); \
            WRITE_sRv2_SIZE1(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 12, _i2C, _1MMA_ * 12); \
            WRITE_sRv2_SIZE1(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 13, _i2C, _1MMA_ * 13); \
            WRITE_sRv2_SIZE1(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 14, _i2C, _1MMA_ * 14); \
            WRITE_sRv2_SIZE1(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 15, _i2C, _1MMA_ * 15); \
        }

/////////////////////////
// tile_n_per_warp = 16
/////////////////////////

#define WRITE_sRv2_1x2(_sm_base_v2, _sRv2_write, _i2C) \
        { \
            WRITE_sRv2_SIZE2(_sm_base_v2, _sRv2_write, _i2C, 0); \
        }

#define WRITE_sRv2_2x2(_sm_base_v2, _sRv2_write, _i2C) \
        { \
            WRITE_sRv2_SIZE2(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 0, _i2C, _2MMA_ * 0); \
            WRITE_sRv2_SIZE2(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 1, _i2C, _2MMA_ * 1); \
        }

#define WRITE_sRv2_4x2(_sm_base_v2, _sRv2_write, _i2C) \
        { \
            WRITE_sRv2_SIZE2(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 0, _i2C, _2MMA_ * 0); \
            WRITE_sRv2_SIZE2(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 1, _i2C, _2MMA_ * 1); \
            WRITE_sRv2_SIZE2(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 2, _i2C, _2MMA_ * 2); \
            WRITE_sRv2_SIZE2(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 3, _i2C, _2MMA_ * 3); \
        }

#define WRITE_sRv2_8x2(_sm_base_v2, _sRv2_write, _i2C) \
        { \
            WRITE_sRv2_SIZE2(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 0, _i2C, _2MMA_ * 0); \
            WRITE_sRv2_SIZE2(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 1, _i2C, _2MMA_ * 1); \
            WRITE_sRv2_SIZE2(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 2, _i2C, _2MMA_ * 2); \
            WRITE_sRv2_SIZE2(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 3, _i2C, _2MMA_ * 3); \
            WRITE_sRv2_SIZE2(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 4, _i2C, _2MMA_ * 4); \
            WRITE_sRv2_SIZE2(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 5, _i2C, _2MMA_ * 5); \
            WRITE_sRv2_SIZE2(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 6, _i2C, _2MMA_ * 6); \
            WRITE_sRv2_SIZE2(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 7, _i2C, _2MMA_ * 7); \
        }

#define WRITE_sRv2_16x2(_sm_base_v2, _sRv2_write, _i2C) \
        { \
            WRITE_sRv2_SIZE2(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 0,  _i2C, _2MMA_ * 0); \
            WRITE_sRv2_SIZE2(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 1,  _i2C, _2MMA_ * 1); \
            WRITE_sRv2_SIZE2(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 2,  _i2C, _2MMA_ * 2); \
            WRITE_sRv2_SIZE2(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 3,  _i2C, _2MMA_ * 3); \
            WRITE_sRv2_SIZE2(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 4,  _i2C, _2MMA_ * 4); \
            WRITE_sRv2_SIZE2(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 5,  _i2C, _2MMA_ * 5); \
            WRITE_sRv2_SIZE2(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 6,  _i2C, _2MMA_ * 6); \
            WRITE_sRv2_SIZE2(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 7,  _i2C, _2MMA_ * 7); \
            WRITE_sRv2_SIZE2(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 8,  _i2C, _2MMA_ * 8); \
            WRITE_sRv2_SIZE2(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 9,  _i2C, _2MMA_ * 9); \
            WRITE_sRv2_SIZE2(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 10, _i2C, _2MMA_ * 10); \
            WRITE_sRv2_SIZE2(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 11, _i2C, _2MMA_ * 11); \
            WRITE_sRv2_SIZE2(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 12, _i2C, _2MMA_ * 12); \
            WRITE_sRv2_SIZE2(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 13, _i2C, _2MMA_ * 13); \
            WRITE_sRv2_SIZE2(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 14, _i2C, _2MMA_ * 14); \
            WRITE_sRv2_SIZE2(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 15, _i2C, _2MMA_ * 15); \
        }

/////////////////////////
// tile_n_per_warp = 32
/////////////////////////

#define WRITE_sRv2_1x4(_sm_base_v2, _sRv2_write, _i2C) \
        { \
            WRITE_sRv2_SIZE4(_sm_base_v2, _sRv2_write, _i2C, 0); \
        }

#define WRITE_sRv2_2x4(_sm_base_v2, _sRv2_write, _i2C) \
        { \
            WRITE_sRv2_SIZE4(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 0, _i2C, _4MMA_ * 0); \
            WRITE_sRv2_SIZE4(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 1, _i2C, _4MMA_ * 1); \
        }

#define WRITE_sRv2_4x4(_sm_base_v2, _sRv2_write, _i2C) \
        { \
            WRITE_sRv2_SIZE4(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 0, _i2C, _4MMA_ * 0); \
            WRITE_sRv2_SIZE4(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 1, _i2C, _4MMA_ * 1); \
            WRITE_sRv2_SIZE4(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 2, _i2C, _4MMA_ * 2); \
            WRITE_sRv2_SIZE4(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 3, _i2C, _4MMA_ * 3); \
        }

#define WRITE_sRv2_8x4(_sm_base_v2, _sRv2_write, _i2C) \
        { \
            WRITE_sRv2_SIZE4(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 0, _i2C, _4MMA_ * 0); \
            WRITE_sRv2_SIZE4(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 1, _i2C, _4MMA_ * 1); \
            WRITE_sRv2_SIZE4(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 2, _i2C, _4MMA_ * 2); \
            WRITE_sRv2_SIZE4(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 3, _i2C, _4MMA_ * 3); \
            WRITE_sRv2_SIZE4(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 4, _i2C, _4MMA_ * 4); \
            WRITE_sRv2_SIZE4(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 5, _i2C, _4MMA_ * 5); \
            WRITE_sRv2_SIZE4(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 6, _i2C, _4MMA_ * 6); \
            WRITE_sRv2_SIZE4(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 7, _i2C, _4MMA_ * 7); \
        }

#define WRITE_sRv2_16x4(_sm_base_v2, _sRv2_write, _i2C) \
        { \
            WRITE_sRv2_SIZE4(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 0,  _i2C, _4MMA_ * 0); \
            WRITE_sRv2_SIZE4(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 1,  _i2C, _4MMA_ * 1); \
            WRITE_sRv2_SIZE4(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 2,  _i2C, _4MMA_ * 2); \
            WRITE_sRv2_SIZE4(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 3,  _i2C, _4MMA_ * 3); \
            WRITE_sRv2_SIZE4(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 4,  _i2C, _4MMA_ * 4); \
            WRITE_sRv2_SIZE4(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 5,  _i2C, _4MMA_ * 5); \
            WRITE_sRv2_SIZE4(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 6,  _i2C, _4MMA_ * 6); \
            WRITE_sRv2_SIZE4(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 7,  _i2C, _4MMA_ * 7); \
            WRITE_sRv2_SIZE4(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 8,  _i2C, _4MMA_ * 8); \
            WRITE_sRv2_SIZE4(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 9,  _i2C, _4MMA_ * 9); \
            WRITE_sRv2_SIZE4(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 10, _i2C, _4MMA_ * 10); \
            WRITE_sRv2_SIZE4(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 11, _i2C, _4MMA_ * 11); \
            WRITE_sRv2_SIZE4(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 12, _i2C, _4MMA_ * 12); \
            WRITE_sRv2_SIZE4(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 13, _i2C, _4MMA_ * 13); \
            WRITE_sRv2_SIZE4(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 14, _i2C, _4MMA_ * 14); \
            WRITE_sRv2_SIZE4(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 15, _i2C, _4MMA_ * 15); \
        }

/////////////////////////
// tile_n_per_warp = 64
/////////////////////////

#define WRITE_sRv2_1x8(_sm_base_v2, _sRv2_write, _i2C) \
        { \
            WRITE_sRv2_SIZE8(_sm_base_v2, _sRv2_write, _i2C, 0); \
        }

#define WRITE_sRv2_2x8(_sm_base_v2, _sRv2_write, _i2C) \
        { \
            WRITE_sRv2_SIZE8(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 0, _i2C, _8MMA_ * 0); \
            WRITE_sRv2_SIZE8(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 1, _i2C, _8MMA_ * 1); \
        }

#define WRITE_sRv2_4x8(_sm_base_v2, _sRv2_write, _i2C) \
        { \
            WRITE_sRv2_SIZE8(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 0, _i2C, _8MMA_ * 0); \
            WRITE_sRv2_SIZE8(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 1, _i2C, _8MMA_ * 1); \
            WRITE_sRv2_SIZE8(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 2, _i2C, _8MMA_ * 2); \
            WRITE_sRv2_SIZE8(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 3, _i2C, _8MMA_ * 3); \
        }

#define WRITE_sRv2_8x8(_sm_base_v2, _sRv2_write, _i2C) \
        { \
            WRITE_sRv2_SIZE8(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 0, _i2C, _8MMA_ * 0); \
            WRITE_sRv2_SIZE8(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 1, _i2C, _8MMA_ * 1); \
            WRITE_sRv2_SIZE8(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 2, _i2C, _8MMA_ * 2); \
            WRITE_sRv2_SIZE8(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 3, _i2C, _8MMA_ * 3); \
            WRITE_sRv2_SIZE8(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 4, _i2C, _8MMA_ * 4); \
            WRITE_sRv2_SIZE8(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 5, _i2C, _8MMA_ * 5); \
            WRITE_sRv2_SIZE8(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 6, _i2C, _8MMA_ * 6); \
            WRITE_sRv2_SIZE8(_sm_base_v2, _sRv2_write + TILE_M_PER_SUB_MMA * TILE_N_V2_PER_CTA * 7, _i2C, _8MMA_ * 7); \
        }



// Licensed to the Apache Software Foundation (ASF) under one
// or more contributor license agreements.  See the NOTICE file
// distributed with this work for additional information
// regarding copyright ownership.  The ASF licenses this file
// to you under the Apache License, Version 2.0 (the
// "License"); you may not use this file except in compliance
// with the License.  You may obtain a copy of the License at
//
//   http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing,
// software distributed under the License is distributed on an
// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
// KIND, either express or implied.  See the License for the
// specific language governing permissions and limitations
// under the License.

/////////////////////////////////////////////////////
// common write shared memory macros
/////////////////////////////////////////////////////

#define WRITE_sUv4_SIZE_16TH(_sm_base_v4, _sm_off, _reg) \
        { \
            if(tid < ( CTA_SIZE_IN_THD / 16 ))  \
                _sm_base_v4[_sm_off] = _reg[0]; \
        }

#define WRITE_sUv4_SIZE_8TH(_sm_base_v4, _sm_off, _reg) \
        { \
            if(tid < ( CTA_SIZE_IN_THD / 8 ))  \
                _sm_base_v4[_sm_off] = _reg[0]; \
        }

#define WRITE_sUv4_SIZE_QTR(_sm_base_v4, _sm_off, _reg) \
        { \
            if(tid < ( CTA_SIZE_IN_THD / 4 ))  \
                _sm_base_v4[_sm_off] = _reg[0]; \
        }

#define WRITE_sUv4_SIZE_HALF(_sm_base_v4, _sm_off, _reg) \
        { \
            if(tid < ( CTA_SIZE_IN_THD / 2 ))  \
                _sm_base_v4[_sm_off] = _reg[0]; \
        }

#define WRITE_sUv4_SIZE1(_sm_base_v4, _sm_off, _reg) \
        { \
            _sm_base_v4[_sm_off] = _reg[0]; \
        }

#define WRITE_sUv4_SIZE2(_sm_base_v4, _sm_off, _reg) \
        { \
            _sm_base_v4[_sm_off] = _reg[0]; \
            _sm_base_v4[_sm_off + CTA_SIZE_IN_THD * 1] = _reg[1]; \
        }

#define WRITE_sUv4_SIZE4(_sm_base_v4, _sm_off, _reg) \
        { \
            _sm_base_v4[_sm_off] = _reg[0]; \
            _sm_base_v4[_sm_off + CTA_SIZE_IN_THD * 1] = _reg[1]; \
            _sm_base_v4[_sm_off + CTA_SIZE_IN_THD * 2] = _reg[2]; \
            _sm_base_v4[_sm_off + CTA_SIZE_IN_THD * 3] = _reg[3]; \
        }

#define WRITE_sUv4_SIZE8(_sm_base_v4, _sm_off, _reg) \
        { \
            _sm_base_v4[_sm_off] = _reg[0]; \
            _sm_base_v4[_sm_off + CTA_SIZE_IN_THD * 1] = _reg[1]; \
            _sm_base_v4[_sm_off + CTA_SIZE_IN_THD * 2] = _reg[2]; \
            _sm_base_v4[_sm_off + CTA_SIZE_IN_THD * 3] = _reg[3]; \
            _sm_base_v4[_sm_off + CTA_SIZE_IN_THD * 4] = _reg[4]; \
            _sm_base_v4[_sm_off + CTA_SIZE_IN_THD * 5] = _reg[5]; \
            _sm_base_v4[_sm_off + CTA_SIZE_IN_THD * 6] = _reg[6]; \
            _sm_base_v4[_sm_off + CTA_SIZE_IN_THD * 7] = _reg[7]; \
        }

#define WRITE_sUv4_SIZE16(_sm_base_v4, _sm_off, _reg) \
        { \
            _sm_base_v4[_sm_off] = _reg[0]; \
            _sm_base_v4[_sm_off + CTA_SIZE_IN_THD * 1]  = _reg[1]; \
            _sm_base_v4[_sm_off + CTA_SIZE_IN_THD * 2]  = _reg[2]; \
            _sm_base_v4[_sm_off + CTA_SIZE_IN_THD * 3]  = _reg[3]; \
            _sm_base_v4[_sm_off + CTA_SIZE_IN_THD * 4]  = _reg[4]; \
            _sm_base_v4[_sm_off + CTA_SIZE_IN_THD * 5]  = _reg[5]; \
            _sm_base_v4[_sm_off + CTA_SIZE_IN_THD * 6]  = _reg[6]; \
            _sm_base_v4[_sm_off + CTA_SIZE_IN_THD * 7]  = _reg[7]; \
            _sm_base_v4[_sm_off + CTA_SIZE_IN_THD * 8]  = _reg[8]; \
            _sm_base_v4[_sm_off + CTA_SIZE_IN_THD * 9]  = _reg[9]; \
            _sm_base_v4[_sm_off + CTA_SIZE_IN_THD * 10] = _reg[10]; \
            _sm_base_v4[_sm_off + CTA_SIZE_IN_THD * 11] = _reg[11]; \
            _sm_base_v4[_sm_off + CTA_SIZE_IN_THD * 12] = _reg[12]; \
            _sm_base_v4[_sm_off + CTA_SIZE_IN_THD * 13] = _reg[13]; \
            _sm_base_v4[_sm_off + CTA_SIZE_IN_THD * 14] = _reg[14]; \
            _sm_base_v4[_sm_off + CTA_SIZE_IN_THD * 15] = _reg[15]; \
        }

////////////////////////////////////////////////////
// read shared memory macros
////////////////////////////////////////////////////

#define REG_sAv1_SIZE   (TILE_M_V1_PER_THD)
#define REG_sBv1_SIZE   (TILE_N_V1_PER_THD)

#define READ_sUv1_SIZE1(_reg, _reg_off, _smp_base_v1, _sUv1_read) \
        { \
            LDSM_ROW_X1_INST(_reg[_reg_off], _smp_base_v1 + _INT_TO_BYTE_ * (_sUv1_read) ); \
        }

#define READ_sUv1_SIZE2(_reg, _reg_off, _smp_base_v1, _sUv1_read) \
        { \
            LDSM_ROW_X2_INST(_reg[_reg_off], _reg[_reg_off + 1], _smp_base_v1 + _INT_TO_BYTE_ * (_sUv1_read) ); \
        }

#define READ_sUv1_SIZE4(_reg, _reg_off, _smp_base_v1, _sUv1_read) \
        { \
            LDSM_ROW_X4_INST(_reg[_reg_off], _reg[_reg_off + 1], _reg[_reg_off + 2], _reg[_reg_off + 3], _smp_base_v1 + _INT_TO_BYTE_ * (_sUv1_read) ); \
        }

#define READ_sUv1_1x1(_reg, _smp_base_v1, _sUv1_read) \
        { \
            READ_sUv1_SIZE1(_reg, 0, _smp_base_v1, _sUv1_read); \
        }

#define READ_sUv1_2x1(_reg, _smp_base_v1, _sUv1_read) \
        { \
            READ_sUv1_SIZE1(_reg, 0, _smp_base_v1, _sUv1_read); \
            READ_sUv1_SIZE1(_reg, 1, _smp_base_v1, _sUv1_read + TILE_K_V4_PER_CTA * (WARP_SIZE_IN_THD / 4) ); \
        }

#define READ_sUv1_1x2(_reg, _smp_base_v1, _sUv1_read) \
        { \
            READ_sUv1_SIZE2(_reg, 0, _smp_base_v1, _sUv1_read); \
        }

#define READ_sUv1_2x2(_reg, _smp_base_v1, _sUv1_read) \
        { \
            READ_sUv1_SIZE2(_reg, 0, _smp_base_v1, _sUv1_read); \
            READ_sUv1_SIZE2(_reg, 2, _smp_base_v1, _sUv1_read + TILE_K_V4_PER_CTA * (WARP_SIZE_IN_THD / 2) ); \
        }

#define READ_sUv1_1x4(_reg, _smp_base_v1, _sUv1_read) \
        { \
            READ_sUv1_SIZE4(_reg, 0, _smp_base_v1, _sUv1_read); \
        }

#define READ_sUv1_2x4(_reg, _smp_base_v1, _sUv1_read) \
        { \
            READ_sUv1_SIZE4(_reg, 0, _smp_base_v1, _sUv1_read); \
            READ_sUv1_SIZE4(_reg, 4, _smp_base_v1, _sUv1_read + TILE_K_V4_PER_CTA * WARP_SIZE_IN_THD); \
        }

#define READ_sUv1_4x4(_reg, _smp_base_v1, _sUv1_read) \
        { \
            READ_sUv1_SIZE4(_reg, 0,  _smp_base_v1, _sUv1_read); \
            READ_sUv1_SIZE4(_reg, 4,  _smp_base_v1, _sUv1_read + TILE_K_V4_PER_CTA * WARP_SIZE_IN_THD * 1); \
            READ_sUv1_SIZE4(_reg, 8,  _smp_base_v1, _sUv1_read + TILE_K_V4_PER_CTA * WARP_SIZE_IN_THD * 2); \
            READ_sUv1_SIZE4(_reg, 12, _smp_base_v1, _sUv1_read + TILE_K_V4_PER_CTA * WARP_SIZE_IN_THD * 3); \
        }

#define READ_sUv1_1x8(_reg, _smp_base_v1, _sUv1_read) \
        { \
            READ_sUv1_SIZE4(_reg, 0, _smp_base_v1, _sUv1_read); \
            READ_sUv1_SIZE4(_reg, 4, _smp_base_v1, _sUv1_read + TILE_K_V4_PER_CTA * WARP_SIZE_IN_THD); \
        }



#define MMA_INSTS(_C, _A, _B)           MMA_INST_8x1(_C, _A, _B)

#define READ_sAv1(_A, _sm_base_v1, _sAv1_read)          READ_sUv1_2x4(_A, _sm_base_v1, _sAv1_read)
#define READ_sBv1(_B, _sm_base_v1, _sBv1_read)          READ_sUv1_1x1(_B, _sm_base_v1, _sBv1_read)

#define WRITE_sRv2(_sm_base_v1, _sRv1_write_base, _C)   WRITE_sRv2_8x1(_sm_base_v1, _sRv1_write_base, _C)

#define LOAD_dAv4(_regA, _dA, _dAv4_off, _in_c_v16_id, _flt_hw_bid)      LOAD_dAv4_SIZE2(_regA, _dA, _dAv4_off, _in_c_v16_id, _flt_hw_bid)
#define WRITE_sAv4(_sm_base_v4, _sm_off, _reg)                          WRITE_sUv4_SIZE2(_sm_base_v4, _sm_off, _reg)

#define LOAD_dBv4(_regB, _dB, _dBv4_off, _flt_c_v16_id, _flt_n_valid)    LOAD_dBv4_SIZE1(_regB, _dB, _dBv4_off, _flt_c_v16_id, _flt_n_valid)
#define WRITE_sBv4(_sm_base_v4, _sm_off, _reg)                          WRITE_sUv4_SIZE1(_sm_base_v4, _sm_off, _reg)

#define FLT_SIZE3

#define cvtOutData(_outData,cvtData0,cvtData1,cvtData2,cvtData3,cvtData4,cvtData5,cvtData6,cvtData7){ \
	    asm volatile("cvt.pack.sat.s8.s32.b32 %0, %1, %2, 0;\n" : "=r"(cvtData4) : "r"(cvtData6), "r"(cvtData4)); \
	    asm volatile("cvt.pack.sat.s8.s32.b32 %0, %1, %2, 0;\n" : "=r"(cvtData5) : "r"(cvtData7), "r"(cvtData5)); \
	    asm volatile("cvt.pack.sat.s8.s32.b32 %0, %1, %2, %3;\n": "=r"(cvtData0) : "r"(cvtData2), "r"(cvtData0), "r"(cvtData4)); \
	    asm volatile("cvt.pack.sat.s8.s32.b32 %0, %1, %2, %3;\n": "=r"(cvtData1) : "r"(cvtData3), "r"(cvtData1), "r"(cvtData5)); \
	    _outData.x = cvtData0; _outData.y = cvtData1; \
}

/*,intMin,intMax){ */
#define quantOutData(_C, _fC, _outInt8Scale){ \
	   _C.x = __float2int_rn(_fC[0]*_outInt8Scale); \
	   _C.y = __float2int_rn(_fC[1]*_outInt8Scale); \
	   _C.z = __float2int_rn(_fC[2]*_outInt8Scale); \
	   _C.w = __float2int_rn(_fC[3]*_outInt8Scale); \
}


#define deQuantData(_fC, _Rv4, _descale){ \
	    _fC[0] = _Rv4.x * _descale.x; \
	    _fC[1] = _Rv4.y * _descale.y; \
	    _fC[2] = _Rv4.z * _descale.z; \
	    _fC[3] = _Rv4.w * _descale.w; \
}

#define LOAD_SCALE(_scale, _d_scale){ \
    if( dCv4_x_valid && dCv4_y_valid ){ \
        _scale = ((float4*)_d_scale)[grp_id * num_flt_per_grp_pad_v4 + dCv4_idx]; \
        _scale.x *= in_scale; \
        _scale.y *= in_scale; \
        _scale.z *= in_scale; \
        _scale.w *= in_scale; \
    } \
}

#define packchar4(_outData, x, y, z, w){ \
    if (x>127)	x = 127;                 \
    if (x<-128) x = -128;                \
    if (y>127)	y = 127;                 \
    if (y<-128) y = -128;                \
    if (z>127)	z = 127;                 \
    if (z<-128) z = -128;                \
    if (w>127)	w = 127;                 \
    if (w<-128) w = -128;                \
    x = (0xffu & (int8_t)x);             \
    y = (0xffu & (int8_t)y) << 8;        \
    z = (0xffu & (int8_t)z) << 16;       \
    w = (0xffu & (int8_t)w) << 24;         \
    _outData = w | z | y | x;/*(x,y,z,w)*/\
}



// Licensed to the Apache Software Foundation (ASF) under one
// or more contributor license agreements.  See the NOTICE file
// distributed with this work for additional information
// regarding copyright ownership.  The ASF licenses this file
// to you under the Apache License, Version 2.0 (the
// "License"); you may not use this file except in compliance
// with the License.  You may obtain a copy of the License at
//
//   http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing,
// software distributed under the License is distributed on an
// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
// KIND, either express or implied.  See the License for the
// specific language governing permissions and limitations
// under the License.

#if defined(ENABLE_FUSE)

#define OUTPUT_PRC_HALF(_Rv1) \
        { \
            if( dCv4_x_valid && dCv4_y_valid ) \
            { \
                ((unsigned int*)dC)[ concatV4_off + dCv4_off ] = _Rv1; \
            } \
        }

#else

#define OUTPUT_PRC_HALF(_Rv1) \
        { \
            if( dCv4_x_valid && dCv4_y_valid ) \
            { \
                ((int*)dC)[ dCv4_off ] = _Rv1; \
            } \
        }
#endif

#define ADD_BIAS_V4(_fR, _has_bias, _bias) \
        { \
                printf("in bias %d, %d, %d", _has_bias, dCv4_x_valid, dCv4_y_valid); \
            if( _has_bias && dCv4_x_valid && dCv4_y_valid ) \
            { \
	            int4  _biasV4 = ((int4 *) _bias) [grp_id * num_flt_per_grp_pad_v4 + dCv4_idx]; \
	            float* _fBias = (float *) &_biasV4; \
                \
	            for(int i = 0; i < _INT4_TO_4INT_; i++) \
                { \
	            _fR[i] = _fR[i] + _fBias[i]; \
		} \
            } \
        }

#define FUSE_RELU_V4(_fR, _has_relu) \
        { \
	        if( _has_relu && dCv4_x_valid  && dCv4_y_valid ) \
            { \
		        if(_has_relu == 1) \
                { \
                    \
                    _Pragma("unroll") \
	                for(int i = 0; i < _INT4_TO_4INT_; i++) \
                    { \
	                    _fR[i] = MAX(_fR[i], 0); \
	                } \
	            } \
                else if(_has_relu == 2) \
                { \
                    _Pragma("unroll") \
	                for(int i = 0; i < _INT4_TO_4INT_; i++) \
                    { \
			    _fR[i] = __expf(_fR[i]) / (1.f + __expf(_fR[i])); \
	            } \
	        } \
            } \
        }

#define FUSE_CLIP_V4(_fR, _has_clip, _clip_max, _clip_min) \
        { \
	        if( _has_clip && dCv4_x_valid  && dCv4_y_valid ) { \
                \
                _Pragma("unroll") \
	            for(int i = 0; i < _INT4_TO_4INT_; i++) \
                    { \
			_fR[i] = MIN(_fR[i], _clip_max); \
			_fR[i] = MAX(_fR[i], _clip_min); \
		    } \
	        } \
        }

#define FUSE_PRELU_V4(_fR, _has_prelu, _prelu, _leaky) \
        { \
	        if( _has_prelu && dCv4_x_valid  && dCv4_y_valid ) { \
                \
       	        if(_has_prelu == 1) \
                { \
                    _Pragma("unroll") \
	                for(int i = 0; i < _INT4_TO_4INT_; i++) \
                    { \
			    if(_fR[i] < 0.f)    _fR[i] *= _leaky; \
	                } \
	            } \
                \
	        else if(_has_prelu == 2) \
                { \
	                int4 _scale_v4 = ( (int4 *) _prelu) [grp_id * num_flt_per_grp_pad_v4 + dCv4_idx]; \
	                float *_scale  = (float *) &_scale_v4; \
                    \
                    _Pragma("unroll") \
	                for(int i = 0; i < _INT4_TO_4INT_; i++) \
                    { \
	            	    if( _fR[i] < 0 )   _fR[i] *= _scale[i]; \
	                } \
	            } \
                \
	        else if(_has_prelu == 3) \
                { \
                    int4 _scale_v4 = ((int4  *) _prelu) [dCv4_off]; \
	            float* _scale  = (float *) &_scale_v4; \
                    \
                    _Pragma("unroll") \
	                for(int i = 0; i < _INT4_TO_4INT_; i++) \
                    { \
	            	    if( _fR[i] < 0 )   _fR[i] *= _scale[i]; \
	            } \
	        } \
		\
	        } \
        }

#define FUSE_ELT_V4(_fR, _has_elt, _pre_data) \
        { \
	        if( _has_elt && dCv4_x_valid && dCv4_y_valid ) \
            { \
	            int  _elt_v4 = ((int *)   _pre_data) [dCv4_off]; \
	            int8_t *_elt = (int8_t *) &_elt_v4; \
                \
                _Pragma("unroll") \
	            for(int i = 0; i < _INT4_TO_4INT_; i++){ \
			_fR[i] += (int)_elt[i] * pre_scale; \
	            } \
	        } \
        }

#define SET_CONCAT_OFF_V4(_has_concat, _concatV4_off) \
        { \
	        if( _has_concat && dCv4_x_valid && dCv4_y_valid ) \
            { \
	            dCv4_off = concat_offset_v16 + dCv4_idy * concat_stride_v16 + dCv4_base + dCv4_idx; \
	        } \
        }

#define JIT_FUSE_RELU_V4(_fR) \
        { \
            _Pragma("unroll") \
            for(int i = 0; i < _INT4_TO_4INT_; i++) \
            { \
                _fR[i] = MAX(_fR[i], 0); \
            } \
        }

#define JIT_FUSE_SIGMOID_V4(_fR) \
        { \
            _Pragma("unroll") \
            for(int i = 0; i < _INT4_TO_4INT_; i++) \
            { \
              _fR[i] = __expf(_fR[i]) / (1.f + __expf(_fR[i])); \
            } \
        }

#define JIT_FUSE_CLIP_V4(_fR, _clip_max, _clip_min) \
        { \
                _Pragma("unroll") \
	            for(int i = 0; i < _INT4_TO_4INT_; i++) \
                    { \
			_fR[i] = MIN(_fR[i], _clip_max); \
			_fR[i] = MAX(_fR[i], _clip_min); \
		    } \
        }

#define JIT_FUSE_PRELU_V4(_fR, _has_prelu, _prelu) \
        { \
            if(_has_prelu == 2) \
                { \
	                int4 _scale_v4 = ( (int4 *) _prelu) [grp_id * num_flt_per_grp_pad_v4 + dCv4_idx]; \
	                float *_scale  = (float *) &_scale_v4; \
                    \
                    _Pragma("unroll") \
	                for(int i = 0; i < _INT4_TO_4INT_; i++) \
                    { \
	            	    if( _fR[i] < 0 )   _fR[i] *= _scale[i]; \
	                } \
	            } \
                \
	        else if(_has_prelu == 3) \
                { \
                    int4 _scale_v4 = ((int4  *) _prelu) [dCv4_off]; \
	            float* _scale  = (float *) &_scale_v4; \
                    \
                    _Pragma("unroll") \
	                for(int i = 0; i < _INT4_TO_4INT_; i++) \
                    { \
	            	    if( _fR[i] < 0 )   _fR[i] *= _scale[i]; \
	            } \
	        } \
        }

#define JIT_FUSE_LEAKY_V4(_fR, _leaky) \
        { \
            _Pragma("unroll") \
            for(int i = 0; i < _INT4_TO_4INT_; i++) \
            { \
			    if(_fR[i] < 0.f)    _fR[i] *= _leaky; \
            } \
        }

#define JIT_FUSE_ELT_V4(_fR, _pre_data) \
        { \
	            int  _elt_v4 = ((int *)   _pre_data) [dCv4_off]; \
	            int8_t *_elt = (int8_t *) &_elt_v4; \
                \
                _Pragma("unroll") \
	            for(int i = 0; i < _INT4_TO_4INT_; i++){ \
			_fR[i] += (int)_elt[i] * pre_scale; \
	            } \
        }

#define JIT_SET_CONCAT_OFF_V4(_concatV4_off) \
        { \
            dCv4_off = concat_offset_v16 + dCv4_idy * concat_stride_v16 + dCv4_base + dCv4_idx; \
        }


extern "C" {

// Licensed to the Apache Software Foundation (ASF) under one
// or more contributor license agreements.  See the NOTICE file
// distributed with this work for additional information
// regarding copyright ownership.  The ASF licenses this file
// to you under the Apache License, Version 2.0 (the
// "License"); you may not use this file except in compliance
// with the License.  You may obtain a copy of the License at
//
//   http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing,
// software distributed under the License is distributed on an
// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
// KIND, either express or implied.  See the License for the
// specific language governing permissions and limitations
// under the License.

#if defined(ENABLE_SPLITK)
__global__ void __launch_bounds__(CTA_SIZE_IN_THD) KERNEL_NAME(SPK_KPARAM_LIST)
#elif defined(ENABLE_FUSE) || defined(ENABLE_SPLITF)
__global__ void __launch_bounds__(CTA_SIZE_IN_THD) KERNEL_NAME(TOTAL_KPARAM_LIST)
#endif
{
#if (__CUDA_ARCH__ >= 750) && (__CUDACC_VER_MAJOR__ * 1000 + __CUDACC_VER_MINOR__ * 10 >= 10020)

    int4 Cv4[Cv4_ITEMS_PER_THD];

    int2 * i2C = (int2 *) Cv4;
    int *    C = (int *)  Cv4;

    for (int i = 0; i < C_ITEMS_PER_THD; i++) { C[i] = _ZERO_; }

    int4  Rv4[INTER_SET_REDUCE_RATIO];


    uint tid       =  threadIdx.x;

    uint local_tid =  tid & 0x1f;

    uint set_tid   =  tid & (SET_SIZE_IN_THD - 1);

    uint set_id    = (tid >> SET_SIZE_IN_BITS) & 0x7;

    uint set_widx  = (set_tid >>  WARP_SIZE_IN_BITS) & (SET_SIZE_X_IN_WARP - 1);
    uint set_widy  =  set_tid >> (WARP_SIZE_IN_BITS  +  SET_SIZE_X_IN_BITS);
 
    uint ldg_idx   =  tid % TILE_K_V16_PER_CTA;
    uint ldg_idy   =  tid / TILE_K_V16_PER_CTA;

#if TILE_K_PER_CTA == 16
    uint sts_idx   =   0;
    uint sts_idy   =   tid;
#elif TILE_K_PER_CTA == 32
    uint sts_idx   = ((tid & 0x1) ^ ((tid & 0xf) >> 3));
    uint sts_idy   =   tid >> 1;
#elif TILE_K_PER_CTA == 64
    uint sts_idx   = ((tid & 0x3) ^ ((tid & 0x1f) >> 3));
    uint sts_idy   =   tid >> 2;
#elif TILE_K_PER_CTA == 128
    uint sts_idx   = ((tid & 0x7) ^ ((tid & 0x3f) >> 3));
    uint sts_idy   =   tid >> 3;
#elif TILE_K_PER_CTA == 256
    uint sts_idx   = ((tid & 0xf) ^ ((tid & 0x7f) >> 4));
    uint sts_idy   =   tid >> 4;
#endif

    uint cta_idx   = blockIdx.y;
    uint cta_idy   = blockIdx.x;

#if defined(ENABLE_SPLITK) && defined(ENABLE_SPLITF)
    uint spk_id    =  blockIdx.z %  splitk;
    uint spf_id    = (blockIdx.z % (splitk * flt_hw)) / splitk;
    uint grp_id    =  blockIdx.z / (splitk * flt_hw);
#elif defined(ENABLE_SPLITK) && !defined(ENABLE_SPLITF)
    uint spk_id    =  blockIdx.z %  splitk;
    uint grp_id    =  blockIdx.z /  splitk;
#elif !defined(ENABLE_SPLITK) && defined(ENABLE_SPLITF)
    uint spf_id    =  blockIdx.z %  flt_hw;
    uint grp_id    =  blockIdx.z /  flt_hw;
#elif defined(ENABLE_FUSE)
    uint grp_id    = blockIdx.z;
#endif

    uint num_chl_per_grp_pad_v16 = num_chl_per_grp_pad >> 4;
    uint num_flt_per_grp_pad_v4 = num_flt_per_grp_pad >> 2;

    uint dCv4_idy   =  cta_idy  * TILE_M_V1_PER_CTA  +
                       tid      / TILE_N_V4_PER_CTA;

    uint dCv4_idx   =  cta_idx  * TILE_N_V4_PER_CTA  +
                       tid      % TILE_N_V4_PER_CTA;

    bool dCv4_x_valid =  (dCv4_idx < num_flt_per_grp_pad_v4) & ((tid / TILE_N_V4_PER_CTA) < TILE_M_PER_CTA);

#if defined(ENABLE_SPLITK) && defined(ENABLE_SPLITF)
    uint dCv4_base  = (spf_id   * splitk + spk_id)  * num_flt_per_grp_pad_v4 * num_grp * out_hw * in_num +
                       grp_id   * num_flt_per_grp_pad_v4;
#elif defined(ENABLE_SPLITK) && !defined(ENABLE_SPLITF)
    uint dCv4_base  =  spk_id   * num_flt_per_grp_pad_v4 * num_grp * out_hw * in_num +
                       grp_id   * num_flt_per_grp_pad_v4;
#elif !defined(ENABLE_SPLITK) && defined(ENABLE_SPLITF)
    uint dCv4_base  =  spf_id   * num_flt_per_grp_pad_v4 * num_grp * out_hw * in_num +
                       grp_id   * num_flt_per_grp_pad_v4;
#elif defined(ENABLE_FUSE)
    uint dCv4_base  =  grp_id   * num_flt_per_grp_pad_v4;
#endif

    uint mma_idx    =  local_tid %  MMA_SIZE_X_IN_THD;
    uint mma_idy    =  local_tid >> MMA_SIZE_X_IN_BITS;

    //可以用宏变量，可扩展的在smem上进行swizzle的原因，是单个mma的一行的size是2的幂，和smem 32 bank size（128B）互为2的幂指的关系
    uint smem_row_write_id  =  (set_widx * TILE_N_V4_PER_WARP) / SMEM_ROW_V4_SIZE;
//FIXME
    // inter_mma_x
    uint smem_row_write_off = ((set_widx * TILE_N_IN_MMA_PER_WARP) ^ (mma_idy  / N_ROWS_PER_SMEM_ROW)
                       ) % MMAs_PER_REDUCE_ROW;//SMEM_ROW_V8_SIZE;//8 int32 才表示有可以放多少个mma的行

    // 2 int32 is v2
    uint sRv2_write =  set_id     * TILE_N_V2_PER_CTA    * TILE_M_V1_PER_CTA  +
                       set_widy   * TILE_N_V2_PER_CTA    * TILE_M_V1_PER_WARP +
                       mma_idy    * TILE_N_V2_PER_CTA    +
                       //smem_row_write_id  * SMEM_ROW_V1_SIZE     +
                       smem_row_write_id  * SMEM_ROW_V2_SIZE     +
                       mma_idx;

    uint red_read_idx =  tid % TILE_N_V4_PER_CTA;
    uint red_read_idy =  tid / TILE_N_V4_PER_CTA; 

    uint smem_row_read_id   =  red_read_idx / SMEM_ROW_V4_SIZE;
    uint smem_row_read_off  =  red_read_idx % SMEM_ROW_V4_SIZE;
    uint inner_mma_read_idx =  smem_row_read_off & (TILE_N_PER_MMA/_4INT_TO_INT4_-1);
    uint inter_mma_read_idx =  smem_row_read_off / (TILE_N_PER_MMA/_4INT_TO_INT4_);

//#define SWIZZLE_GROUP MAX( 1, 4/(CTA_SIZE_IN_THD / TILE_N_V4_PER_CTA) )
    uint sRv4_read[SWIZZLE_GROUP];
    sRv4_read[0]  =   red_read_idy * TILE_N_V4_PER_CTA    +
                      smem_row_read_id  * SMEM_ROW_V4_SIZE +
                    //(((red_read_idy % TILE_M_PER_MMA_HALF) / N_ROWS_PER_SMEM_ROW) ^ smem_row_read_off);//这里可以直接用smem_row_read_off的原因是刚好一个mma_seg是int4
                    //(((red_read_idy / N_ROWS_PER_SMEM_ROW) % (SMEM_ROW_BYTE_SIZE/TILE_N_PER_MMA/sizeof(int))) ^ inter_mma_read_idx) * (TILE_N_PER_MMA/_4INT_TO_INT4_) +
                    (((red_read_idy / N_ROWS_PER_SMEM_ROW) % MMAs_PER_REDUCE_ROW) ^ inter_mma_read_idx) * (TILE_N_PER_MMA/_4INT_TO_INT4_) +
		     inner_mma_read_idx;
#if SWIZZLE_GROUP == 2
    sRv4_read[1] = (red_read_idy+2) * TILE_N_V4_PER_CTA    +
                 smem_row_read_id  * SMEM_ROW_V4_SIZE +
                 ((((red_read_idy+2) / N_ROWS_PER_SMEM_ROW) % MMAs_PER_REDUCE_ROW) ^ inter_mma_read_idx) * (TILE_N_PER_MMA/_4INT_TO_INT4_) +
    	         inner_mma_read_idx;
#elif SWIZZLE_GROUP == 4
    sRv4_read[1] = red_read_idy * TILE_N_V4_PER_CTA    +
                 smem_row_read_id  * SMEM_ROW_V4_SIZE +
                 ((((red_read_idy + 1) / N_ROWS_PER_SMEM_ROW) % MMAs_PER_REDUCE_ROW) ^ inter_mma_read_idx) * (TILE_N_PER_MMA/_4INT_TO_INT4_) +
    	         inner_mma_read_idx;
    sRv4_read[2] = red_read_idy * TILE_N_V4_PER_CTA    +
                 smem_row_read_id  * SMEM_ROW_V4_SIZE +
                 ((((red_read_idy + 2) / N_ROWS_PER_SMEM_ROW) % MMAs_PER_REDUCE_ROW) ^ inter_mma_read_idx) * (TILE_N_PER_MMA/_4INT_TO_INT4_) +
    	         inner_mma_read_idx;
    sRv4_read[3] = red_read_idy * TILE_N_V4_PER_CTA    +
                 smem_row_read_id  * SMEM_ROW_V4_SIZE +
                 ((((red_read_idy + 3) / N_ROWS_PER_SMEM_ROW) % MMAs_PER_REDUCE_ROW) ^ inter_mma_read_idx) * (TILE_N_PER_MMA/_4INT_TO_INT4_) +
    	         inner_mma_read_idx;
#endif

    const int4 ZEROv4 = {0, 0, 0, 0};

#if defined(FLT_SIZE3)
    int flt_hw_id  = 0;
    int flt_hw_bid = 0x1;

    int lut_id     = 0;
#elif defined(FLT_SIZEN)
    int  flt_h_id  = 0;
    int  flt_w_id  = 0;

    int lut_id     = 0;
#endif

#if defined(ENABLE_SPLITK)
    //TODO modify
    int  flt_c_v16_end = chl_lut.idx[spk_id + 1] >> 4;
    int  flt_c_v16_id  = ldg_idx + (chl_lut.idx[spk_id] >> 4);
#elif defined(ENABLE_SPLITF) || defined(ENABLE_FUSE)
    int  flt_c_v16_end = num_chl_per_grp_pad_v16;
    int  flt_c_v16_id  = ldg_idx;
#endif

    bool flt_c_v16_valid  = flt_c_v16_id < flt_c_v16_end;

    int4 reg_dAv4[REG_dAv4_SIZE];
    int4 reg_dBv4[REG_dBv4_SIZE];

#if defined(FLT_SIZE1)
    int     dAv4_off[READ_dAv4_STEPS];
    bool in_hw_valid[READ_dAv4_STEPS];

    for(int i = 0; i < READ_dAv4_STEPS; i++)
    {
        SET_dAv4_BOUND(i, dAv4_off[i], in_hw_valid[i]);
    }
#elif defined(FLT_SIZE3)
    int dAv4_off[READ_dAv4_STEPS];
    int in_hw_mask[READ_dAv4_STEPS];

    for(int i = 0; i < READ_dAv4_STEPS; i++)
    {
        SET_dAv4_BOUND(i, dAv4_off[i], in_hw_mask[i]);
    }
#elif defined(FLT_SIZEN)
    int dAv4_off[READ_dAv4_STEPS];
    int  in_n_id[READ_dAv4_STEPS];
    int  in_h_id[READ_dAv4_STEPS];
    int  in_w_id[READ_dAv4_STEPS];

    int in_h_start[READ_dAv4_STEPS];
    int in_w_start[READ_dAv4_STEPS];

    for(int i = 0; i < READ_dAv4_STEPS; i++)
    {
        SET_dAv4_BOUND(i, dAv4_off[i], in_n_id[i], in_h_start[i], in_w_start[i]);
        in_h_id[i] = in_h_start[i];
        in_w_id[i] = in_w_start[i];
    }
#endif

    int     dBv4_off[READ_dBv4_STEPS];
    bool flt_n_valid[READ_dBv4_STEPS];

    for(int i = 0; i < READ_dBv4_STEPS; i++)
    {
        SET_dBv4_BOUND(i, dBv4_off[i], flt_n_valid[i]);
    }

#if defined(USE_1BUF)
    __shared__ int4 sm_base_v4[SM_BASE_V4_1BUF];
#elif defined(USE_2BUF)
    __shared__ int4 sm_base_v4[SM_BASE_V4_2BUF];
#endif
    int * sm_base_v1 = (int *) sm_base_v4;
    int2 *sm_base_v2 = (int2 *)sm_base_v4;
    
    uint32_t smp_base_v1;

    CVT_SM_PTR(smp_base_v1, sm_base_v1);

    uint sAv4_write =  sts_idy  * TILE_K_V16_PER_CTA + sts_idx;

#if defined(USE_1BUF)
    uint sBv4_write =  sAv4_write + SM_A_V4_1BUF;
#elif defined(USE_2BUF)
    uint sBv4_write =  sAv4_write + SM_A_V4_2BUF;
#endif

    uint lds_idy =  local_tid;
#if TILE_K_PER_CTA == 16
    uint lds_idx =  0;
#elif TILE_K_PER_CTA == 32
    uint lds_idx = ((set_id * TILE_K_V16_PER_SET) & 0x1) ^ ((local_tid / K_ROWS_PER_SMEM_ROW) & 0x1);
#elif TILE_K_PER_CTA == 64
    uint lds_idx = ((set_id * TILE_K_V16_PER_SET) & 0x3) ^ ((local_tid / K_ROWS_PER_SMEM_ROW) & 0x3);
#elif TILE_K_PER_CTA == 128
    uint lds_idx = ((set_id * TILE_K_V16_PER_SET) & 0x7) ^ ((local_tid / K_ROWS_PER_SMEM_ROW) & 0x7);
#elif TILE_K_PER_CTA == 256
    uint lds_idx = ((set_id * TILE_K_V16_PER_SET) & 0xf) ^ (local_tid & 0x7);
#endif

    uint sAv1_read  =  set_widy   * TILE_M_PER_WARP        * TILE_K_V4_PER_CTA +
#if TILE_M_PER_WARP == 8
                      (lds_idy    % WARP_SIZE_IN_THD_QTR)  * TILE_K_V4_PER_CTA +
#elif TILE_M_PER_WARP == 16
                      (lds_idy    % WARP_SIZE_IN_THD_HALF) * TILE_K_V4_PER_CTA +
#elif TILE_M_PER_WARP == 32
                       lds_idy    * TILE_K_V4_PER_CTA      +
#elif TILE_M_PER_WARP == 64 || TILE_M_PER_WARP == 128
                       lds_idy    * TILE_K_V4_PER_CTA      +
#endif
                       lds_idx    * _INT4_TO_4INT_;

    uint sBv1_read  =  set_widx   * TILE_N_PER_WARP        * TILE_K_V4_PER_CTA +
#if TILE_N_PER_WARP == 8
                      (lds_idy    % WARP_SIZE_IN_THD_QTR)  * TILE_K_V4_PER_CTA +
#elif TILE_N_PER_WARP == 16
                      (lds_idy    % WARP_SIZE_IN_THD_HALF) * TILE_K_V4_PER_CTA +
#elif TILE_N_PER_WARP == 32 || TILE_N_PER_WARP == 64
                       lds_idy    * TILE_K_V4_PER_CTA      +
#endif
                       lds_idx    * _INT4_TO_4INT_         +
#if defined(USE_1BUF)
                       SM_A_V1_1BUF;
#elif defined(USE_2BUF)
                       SM_A_V1_2BUF;
#endif

    int db0_sBv1[REG_sBv1_SIZE];
#if TILE_K_PER_SET == 32 || TILE_K_PER_SET == 64
    int db1_sBv1[REG_sBv1_SIZE];
#endif

    int db0_sAv1[REG_sAv1_SIZE];
#if TILE_K_PER_SET == 32 || TILE_K_PER_SET == 64
    int db1_sAv1[REG_sAv1_SIZE];
#endif

#if defined(FLT_SIZE1)
    LOAD_dAv4(reg_dAv4, dA, dAv4_off, flt_c_v16_valid, in_hw_valid);
    LOAD_dBv4(reg_dBv4, dB, dBv4_off, flt_c_v16_valid, flt_n_valid);

    FWD_FLT(flt_c_v16_id, flt_c_v16_valid);
#elif defined(FLT_SIZE3)
    LOAD_dAv4(reg_dAv4, dA, dAv4_off, flt_c_v16_valid, flt_hw_bid);
    LOAD_dBv4(reg_dBv4, dB, dBv4_off, flt_c_v16_valid, flt_n_valid);

    FWD_FLT(flt_hw_id, flt_hw_bid, flt_c_v16_id, flt_c_v16_valid);
    FWD_LUT(lut_id);
#elif defined(FLT_SIZEN)
    LOAD_dAv4(reg_dAv4, dA, dAv4_off, in_n_id, in_h_id, in_w_id);
    LOAD_dBv4(reg_dBv4, dB, dBv4_off, flt_c_v16_valid, flt_n_valid);

    FWD_FLT(flt_h_id, flt_w_id, flt_c_v16_id, flt_c_v16_valid);
    FWD_LUT(lut_id);
#endif

    WRITE_sAv4(sm_base_v4, sAv4_write, reg_dAv4);
    WRITE_sBv4(sm_base_v4, sBv4_write, reg_dBv4);

    __syncthreads();

#if defined(USE_2BUF)
    SWITCH_BUFFER(sAv4_write, SM_A_V4_1BUF, 0);
    SWITCH_BUFFER(sBv4_write, SM_B_V4_1BUF, SM_A_V4_2BUF);
#endif

    READ_sAv1(db0_sAv1, smp_base_v1, sAv1_read);
    READ_sBv1(db0_sBv1, smp_base_v1, sBv1_read);

#if TILE_K_PER_SET == 32 || TILE_K_PER_SET == 64
    FWD_KGROUP_STEP1(sAv1_read);
    FWD_KGROUP_STEP1(sBv1_read);
#endif

#if defined(ENABLE_SPLITK)
    for (uint j = 0; j < kloop_lut.idx[spk_id]; j++)
#elif defined(ENABLE_SPLITF) || defined(ENABLE_FUSE)
    for (uint j = 0; j < kloop_num; j++)
#endif
    {
#if defined(FLT_SIZE1)
        LOAD_dAv4(reg_dAv4, dA, dAv4_off, flt_c_v16_valid, in_hw_valid);
        LOAD_dBv4(reg_dBv4, dB, dBv4_off, flt_c_v16_valid, flt_n_valid);

        FWD_FLT(flt_c_v16_id, flt_c_v16_valid);
#elif defined(FLT_SIZE3)
        LOAD_dAv4(reg_dAv4, dA, dAv4_off, flt_c_v16_valid, flt_hw_bid);
        LOAD_dBv4(reg_dBv4, dB, dBv4_off, flt_c_v16_valid, flt_n_valid);

        FWD_FLT(flt_hw_id, flt_hw_bid, flt_c_v16_id, flt_c_v16_valid);
        FWD_LUT(lut_id);
#elif defined(FLT_SIZEN)
        LOAD_dAv4(reg_dAv4, dA, dAv4_off, in_n_id, in_h_id, in_w_id);
        LOAD_dBv4(reg_dBv4, dB, dBv4_off, flt_c_v16_valid, flt_n_valid);

        FWD_FLT(flt_h_id, flt_w_id, flt_c_v16_id, flt_c_v16_valid);
        FWD_LUT(lut_id);
#endif

#if TILE_K_PER_SET == 32 || TILE_K_PER_SET == 64
        READ_sAv1(db1_sAv1, smp_base_v1, sAv1_read);
        READ_sBv1(db1_sBv1, smp_base_v1, sBv1_read);
#endif

#if TILE_K_PER_SET == 32
        FWD_KGROUP_STEP1(sAv1_read);
        FWD_KGROUP_STEP1(sBv1_read);
#elif TILE_K_PER_SET == 64
        FWD_KGROUP_STEP2(sAv1_read);
        FWD_KGROUP_STEP2(sBv1_read);
#endif

        MMA_INSTS(C, db0_sAv1, db0_sBv1);

#if TILE_K_PER_SET == 64
        READ_sAv1(db0_sAv1, smp_base_v1, sAv1_read);
        READ_sBv1(db0_sBv1, smp_base_v1, sBv1_read);

        FWD_KGROUP_STEP3(sAv1_read);
        FWD_KGROUP_STEP3(sBv1_read);

        MMA_INSTS(C, db1_sAv1, db1_sBv1);

        READ_sAv1(db1_sAv1, smp_base_v1, sAv1_read);
        READ_sBv1(db1_sBv1, smp_base_v1, sBv1_read);

        FWD_KGROUP_STEP4(sAv1_read);
        FWD_KGROUP_STEP4(sBv1_read);
#endif

#if defined(USE_1BUF)
        __syncthreads();
#endif

        WRITE_sAv4(sm_base_v4, sAv4_write, reg_dAv4);
        WRITE_sBv4(sm_base_v4, sBv4_write, reg_dBv4);

#if TILE_K_PER_SET == 32
        MMA_INSTS(C, db1_sAv1, db1_sBv1);
#elif TILE_K_PER_SET == 64
        MMA_INSTS(C, db0_sAv1, db0_sBv1);
#endif

#if defined(USE_2BUF)
        SWITCH_BUFFER(sAv4_write, SM_A_V4_1BUF, 0);
        SWITCH_BUFFER(sBv4_write, SM_B_V4_1BUF, SM_A_V4_2BUF);

        SWITCH_BUFFER(sAv1_read,  SM_A_V1_1BUF, 0);
        SWITCH_BUFFER(sBv1_read,  SM_B_V1_1BUF, SM_A_V1_2BUF);
#endif

        __syncthreads();

        READ_sAv1(db0_sAv1, smp_base_v1, sAv1_read);
        READ_sBv1(db0_sBv1, smp_base_v1, sBv1_read);

#if TILE_K_PER_SET == 32 || TILE_K_PER_SET == 64
        FWD_KGROUP_STEP1(sAv1_read);
        FWD_KGROUP_STEP1(sBv1_read);
#endif

#if TILE_K_PER_SET == 64
        MMA_INSTS(C, db1_sAv1, db1_sBv1);
#endif
    }

    __syncthreads();

    WRITE_sRv2(sm_base_v2, sRv2_write, i2C);

    __syncthreads();

    float4 deScale;
    float *fR = (float *)Rv4;
    int outData;
    for(int s = 0; s < OUTPUT_STEPS; s++)
    {
	//fp16时不用每次swizzle idx，是因为一个cta的线程可以把sub_mma行(即8行)的数据一次加载完，下一个sub_mma块就可以沿用上一次读的索引不变，只需加上偏移即可。
        //READ_sRv4(Rv4, sm_base_v4, sRv4_read);
        READ_sRv4(Rv4, sm_base_v4, sRv4_read[s % SWIZZLE_GROUP]);

#if TILE_K_PER_CTA > TILE_K_PER_SET
        int * Rv1  = (int *) Rv4;
        REDUCE(Rv1);
#endif

        bool dCv4_y_valid = (dCv4_idy  / out_hw) < in_num;
        uint dCv4_off     =  dCv4_base +
                             dCv4_idy  * num_flt_per_grp_pad_v4 * num_grp +
                             dCv4_idx;
        LOAD_SCALE(deScale, d_flt_scale);

        deQuantData(fR, Rv4[0], deScale);
        printf("to fuse");
#if defined(ENABLE_FUSE)
        printf("to bias");
        ADD_BIAS_V4(fR, has_bias, bias);
#endif

#if defined(ENABLE_FUSE)
        uint concatV4_off = 0;

        FUSE_RELU_V4(fR, has_relu);
        FUSE_CLIP_V4(fR, has_clip, clip_max, clip_min);
        FUSE_PRELU_V4(fR, has_prelu, prelu, leaky);

        FUSE_ELT_V4(fR, has_elt, pre_data);
        FUSE_RELU_V4(fR, has_elt_relu);
        FUSE_CLIP_V4(fR, has_elt_clip, elt_clip_max, elt_clip_min);
        FUSE_PRELU_V4(fR, has_elt_prelu, elt_prelu, elt_leaky);

        SET_CONCAT_OFF_V4(has_concat, concatV4_off);
#endif
        quantOutData(Rv4[0], fR, out_scale);
        packchar4(outData, Rv4[0].x, Rv4[0].y, Rv4[0].z, Rv4[0].w);
	

        OUTPUT_PRC_HALF(outData);

        dCv4_idy += OUTPUT_SIZE_Y_IN_THD;
    }

#endif // __CUDA_ARCH__
}


}

// Licensed to the Apache Software Foundation (ASF) under one
// or more contributor license agreements.  See the NOTICE file
// distributed with this work for additional information
// regarding copyright ownership.  The ASF licenses this file
// to you under the Apache License, Version 2.0 (the
// "License"); you may not use this file except in compliance
// with the License.  You may obtain a copy of the License at
//
//   http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing,
// software distributed under the License is distributed on an
// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
// KIND, either express or implied.  See the License for the
// specific language governing permissions and limitations
// under the License.

////////////////////////////////////////
// kernel list macros
////////////////////////////////////////

#undef SPK_KPARAM_LIST
#undef TOTAL_KPARAM_LIST

////////////////////////////////////////
// customized macros
////////////////////////////////////////

#undef TILE_N_PER_CTA
#undef TILE_M_PER_CTA

#undef TILE_K_PER_CTA
#undef TILE_K_PER_SET
#undef TILE_K_PER_WARP

#undef TILE_N_PER_WARP
#undef TILE_M_PER_WARP

#undef KERNEL_NAME

////////////////////////////////////////
// align functions
////////////////////////////////////////

#undef Align
#undef DivUp

#undef Min
#undef Max

////////////////////////////////////////
// boundary check
////////////////////////////////////////

#undef WidthInRange
#undef HeightInRange

////////////////////////////////////////
// constant cta size macros
////////////////////////////////////////

#undef _4CHAR_TO_INT_
#undef _4INT_TO_INT4_
#undef _2INT_TO_INT2_

#undef _2HALF_TO_INT_
#undef _2INT2_TO_INT4_

#undef _C1_
#undef _C2_
#undef _C4_
#undef _C8_
#undef _C16_
#undef _C32_

#undef _1INT_
#undef _2INT_
#undef _4INT_
#undef _8INT_

#undef _1INT4_
#undef _2INT4_
#undef _4INT4_
#undef _8INT4_

#undef _1INT8_
#undef _2INT8_
#undef _4INT8_
#undef _8INT8_

#undef _1HALF_
#undef _2HALF_
#undef _4HALF_
#undef _8HALF_

#undef _1HALF2_
#undef _2HALF2_
#undef _4HALF2_
#undef _8HALF2_

#undef _1MMA_
#undef _2MMA_
#undef _4MMA_
#undef _8MMA_

#undef _HALF_ZERO_

#undef _INT_TO_BYTE_
#undef _INT_TO_2HALF_
#undef _INT2_TO_2HALF2_
#undef _INT2_TO_2INT_

#undef _INT4_TO_INT4_
#undef _INT4_TO_2INT2_
#undef _INT4_TO_4INT_
#undef _INT4_TO_4HALF2_
#undef _INT4_TO_8HALF_

#undef SMEM_ROW_V1_SIZE
#undef SMEM_ROW_V4_SIZE
#undef SMEM_ROW_BYTE_SIZE
#undef SMEM_ROW_BIT_SIZE


////////////////////////////////////////
// mma size macros
////////////////////////////////////////

#undef TILE_M_PER_MMA
#undef TILE_K_PER_MMA
#undef TILE_N_PER_MMA
#undef TILE_M_PER_MMA_HALF

#undef MMA_SIZE_Y_IN_THD
#undef MMA_SIZE_Y_IN_THD

#undef MMA_SIZE_X_IN_BITS

////////////////////////////////////////
// thread / warp / cta size macros
////////////////////////////////////////

#undef WARP_SIZE_IN_THD
#undef WARP_SIZE_IN_BITS

#undef WARP_SIZE_X_IN_THD
#undef WARP_SIZE_Y_IN_THD

#undef SET_SIZE_IN_WARP
#undef SET_SIZE_IN_THD
#undef SET_SIZE_IN_BITS

#undef SET_SIZE_X_IN_WARP
#undef SET_SIZE_Y_IN_WARP

#undef SET_SIZE_X_IN_BITS
#undef SET_SIZE_Y_IN_BITS

#undef CTA_SIZE_IN_WARP
#undef CTA_SIZE_IN_THD
#undef CTA_SIZE_IN_BITS

////////////////////////////////////////
// tiling size macros
////////////////////////////////////////

#undef TILE_M_PER_THD
#undef TILE_N_PER_THD

/////////////////////
// tile m

#undef TILE_M_V1_PER_CTA
#undef TILE_M_V2_PER_CTA
#undef TILE_M_V4_PER_CTA
#undef TILE_M_V8_PER_CTA

#undef TILE_M_V1_PER_WARP
#undef TILE_M_V2_PER_WARP
#undef TILE_M_V4_PER_WARP
#undef TILE_M_V8_PER_WARP

#undef TILE_M_V1_PER_THD
#undef TILE_M_V2_PER_THD
#undef TILE_M_V4_PER_THD
#undef TILE_M_V8_PER_THD

#undef TILE_M_V1_PER_MMA
#undef TILE_M_V2_PER_MMA
#undef TILE_M_V4_PER_MMA
#undef TILE_M_V8_PER_MMA

/////////////////////
// tile k

#undef TILE_K_V1_PER_CTA
#undef TILE_K_V2_PER_CTA
#undef TILE_K_V4_PER_CTA
#undef TILE_K_V8_PER_CTA

#undef TILE_K_V1_PER_WARP
#undef TILE_K_V2_PER_WARP
#undef TILE_K_V4_PER_WARP
#undef TILE_K_V8_PER_WARP

#undef TILE_K_V1_PER_THD
#undef TILE_K_V2_PER_THD
#undef TILE_K_V4_PER_THD
#undef TILE_K_V8_PER_THD

#undef TILE_K_V1_PER_KMA
#undef TILE_K_V2_PER_KMA
#undef TILE_K_V4_PER_KMA
#undef TILE_K_V8_PER_KMA


/////////////////////
// tile n

#undef TILE_N_V1_PER_CTA
#undef TILE_N_V2_PER_CTA
#undef TILE_N_V4_PER_CTA
#undef TILE_N_V8_PER_CTA

#undef TILE_N_V1_PER_WARP
#undef TILE_N_V2_PER_WARP
#undef TILE_N_V4_PER_WARP
#undef TILE_N_V8_PER_WARP

#undef TILE_N_V1_PER_THD
#undef TILE_N_V2_PER_THD
#undef TILE_N_V4_PER_THD
#undef TILE_N_V8_PER_THD

#undef TILE_N_V1_PER_MMA
#undef TILE_N_V2_PER_MMA
#undef TILE_N_V4_PER_MMA
#undef TILE_N_V8_PER_MMA


////////////////////////////////////////
// shared memory size macros
////////////////////////////////////////

#undef OUTPUT_STEPS

#undef N_ROWS_PER_SMEM_ROW
#undef K_ROWS_PER_SMEM_ROW

#undef OUTPUT_SIZE_X_IN_THD
#undef OUTPUT_SIZE_Y_IN_THD

////////////////////////////////////////
// main loop macros
////////////////////////////////////////

#undef C_ITEMS_PER_THD

////////////////////////////////////////
// load A and B from device memory macros
////////////////////////////////////////

#undef REG_dAv4_SIZE

#undef REG_dBv1_SIZE
#undef REG_dBv2_SIZE
#undef REG_dBv4_SIZE

#undef READ_dBv1_STEPS
#undef READ_dBv4_STEPS

#undef SET_dBv1_BOUND
#undef SET_dBv4_BOUND

////////////////////////////////////////
// shared memory size macros
////////////////////////////////////////

#undef USE_1BUF
#undef USE_2BUF

#undef SM_A_SIZE
#undef SM_B_SIZE
#undef SM_C_SIZE

#undef SM_A_1BUF
#undef SM_B_1BUF
#undef SM_C_1BUF

#undef SM_A_2BUF
#undef SM_B_2BUF
#undef SM_C_2BUF

#undef SM_A_V1_1BUF
#undef SM_B_V1_1BUF
#undef SM_C_V1_1BUF

#undef SM_A_V2_1BUF
#undef SM_B_V2_1BUF
#undef SM_C_V2_1BUF

#undef SM_A_V4_1BUF
#undef SM_B_V4_1BUF
#undef SM_C_V4_1BUF

#undef SM_A_V1_2BUF
#undef SM_B_V1_2BUF
#undef SM_C_V1_2BUF

#undef SM_A_V2_2BUF
#undef SM_B_V2_2BUF
#undef SM_C_V2_2BUF

#undef SM_A_V4_2BUF
#undef SM_B_V4_2BUF
#undef SM_C_V4_2BUF

#undef SM_BASE_V4_1BUF
#undef SM_BASE_V4_2BUF

#undef CVT_SM_PTR

#undef FWD_LUT

#undef FWD_FLT
#undef FWD_FLT1
#undef FLT_SIZE1
#undef FWD_FLT3
#undef FLT_SIZE3
#undef FWD_FLTN
#undef FLT_SIZEN

#undef FWD_FLT_SIZE1
#undef FWD_FLT_SIZE2
#undef FWD_FLT_SIZE4
#undef FWD_FLT_SIZE8
#undef FWD_FLT_SIZE16

////////////////////////////////////////
// mma macros
////////////////////////////////////////

#undef MMA_INST_OPCODE
#undef MMA_INST

#undef MMA_INST_ASCEND1
#undef MMA_INST_ASCEND2
#undef MMA_INST_ASCEND4
#undef MMA_INST_ASCEND8

#undef MMA_INST_DESCEND1
#undef MMA_INST_DESCEND2
#undef MMA_INST_DESCEND4
#undef MMA_INST_DESCEND8

#undef MMA_INST_1x1
#undef MMA_INST_1x2
#undef MMA_INST_1x4
#undef MMA_INST_1x8

#undef MMA_INST_2x1
#undef MMA_INST_2x2
#undef MMA_INST_2x4
#undef MMA_INST_2x8

#undef MMA_INST_4x1
#undef MMA_INST_4x2
#undef MMA_INST_4x4
#undef MMA_INST_4x8

#undef MMA_INST_8x1
#undef MMA_INST_8x2
#undef MMA_INST_8x4

#undef MMA_INSTS

/////////////////////////////////////////////////////
// reduce half2 macros
/////////////////////////////////////////////////////

#undef REDUCE_HALF2_SIZE4

#undef REDUCE_HALF2_1x4
#undef REDUCE_HALF2_3x4

#undef REDUCE

/////////////////////////////////////////////////////
// read sRv4 macros
/////////////////////////////////////////////////////

#undef READ_sRv4_SIZE1
#undef READ_sRv4_SIZE2
#undef READ_sRv4_SIZE4

#undef READ_sRv4

/////////////////////////////////////////////////////
// write sRv1 macros
/////////////////////////////////////////////////////

#undef WRITE_sRv2_SIZE1
#undef WRITE_sRv2_SIZE2
#undef WRITE_sRv2_SIZE4
#undef WRITE_sRv2_SIZE8

#undef WRITE_sRv2_1x1
#undef WRITE_sRv2_2x1
#undef WRITE_sRv2_4x1
#undef WRITE_sRv2_8x1
#undef WRITE_sRv2_16x1

#undef WRITE_sRv2_1x2
#undef WRITE_sRv2_2x2
#undef WRITE_sRv2_4x2
#undef WRITE_sRv2_8x2
#undef WRITE_sRv2_16x2

#undef WRITE_sRv2_1x4
#undef WRITE_sRv2_2x4
#undef WRITE_sRv2_4x4
#undef WRITE_sRv2_8x4
#undef WRITE_sRv2_16x4

#undef WRITE_sRv2_1x8
#undef WRITE_sRv2_2x8
#undef WRITE_sRv2_4x8
#undef WRITE_sRv2_8x8

#undef WRITE_sRv2

/////////////////////////////////////////////////////
// common load global memory macros
/////////////////////////////////////////////////////

//////////////////////////
// load dA
//////////////////////////

#undef LOAD_dAv4_SIZE_16TH
#undef LOAD_dAv4_SIZE_8TH
#undef LOAD_dAv4_SIZE_QTR
#undef LOAD_dAv4_SIZE_HALF
#undef LOAD_dAv4_SIZE1
#undef LOAD_dAv4_SIZE2
#undef LOAD_dAv4_SIZE4
#undef LOAD_dAv4_SIZE8
#undef LOAD_dAv4_SIZE16

#undef LOAD_dAv4

#undef SET_dAv4_BOUND 

//////////////////////////
// load dB
//////////////////////////

#undef LOAD_dBv4_SIZE_16TH
#undef LOAD_dBv4_SIZE_8TH
#undef LOAD_dBv4_SIZE_QTR
#undef LOAD_dBv4_SIZE_HALF
#undef LOAD_dBv4_SIZE1
#undef LOAD_dBv4_SIZE2
#undef LOAD_dBv4_SIZE4
#undef LOAD_dBv4_SIZE8
#undef LOAD_dBv4_SIZE16

#undef LOAD_dBv4

#undef SET_dBv4_BOUND 

/////////////////////////////////////////////////////
// common write shared memory macros
/////////////////////////////////////////////////////

#undef SWITCH_BUFFER

#undef FWD_KGROUP_ODD
#undef FWD_KGROUP_EVEN

#undef FWD_KGROUP_STEP1
#undef FWD_KGROUP_STEP2
#undef FWD_KGROUP_STEP3
#undef FWD_KGROUP_STEP4

#undef C_ITEMS_PER_THD
#undef HC_ITEMS_PER_THD
#undef Cv4_ITEMS_PER_THD

//////////////////////////
// write sA & sB
//////////////////////////

#undef WRITE_sUv4_SIZE_16TH
#undef WRITE_sUv4_SIZE_8TH
#undef WRITE_sUv4_SIZE_QTR
#undef WRITE_sUv4_SIZE_HALF
#undef WRITE_sUv4_SIZE1
#undef WRITE_sUv4_SIZE2
#undef WRITE_sUv4_SIZE4
#undef WRITE_sUv4_SIZE8
#undef WRITE_sUv4_SIZE16

#undef WRITE_sAv4
#undef WRITE_sBv4

/////////////////////////////////////////////////////
// read shared memory macros
/////////////////////////////////////////////////////

//////////////////////////
// read sA & sB
//////////////////////////

#undef REG_sAv1_SIZE
#undef REG_sBv1_SIZE

#undef READ_sUv1_SIZE1
#undef READ_sUv1_SIZE2
#undef READ_sUv1_SIZE4

#undef READ_sUv1_1x1
#undef READ_sUv1_2x1

#undef READ_sUv1_1x2
#undef READ_sUv1_2x2

#undef READ_sUv1_1x4
#undef READ_sUv1_2x4

#undef READ_sAv1
#undef READ_sBv1

/////////////////////////////////////////////////////
// precision half output
/////////////////////////////////////////////////////

#undef OUTPUT_PRC_HALF

#undef ADD_BIAS_V4

#undef FUSE_RELU_V4
#undef FUSE_CLIP_V4
#undef FUSE_PRELU_V4
#undef FUSE_ELT_V4

#undef SET_CONCAT_OFF_V4


#undef cvtOutData
#undef quantOutData
#undef deQuantData
#undef LOAD_SCALE
#undef packchar4
#undef TILE_N_IN_MMA_PER_WARP
#undef MMAs_PER_REDUCE_ROW
#undef SWIZZLE_GROUP
#undef MAX
#undef MIN


#undef ENABLE_FUSE


[0;32m[INFO][2021-11-30 11:07:01.402][pplnn.cc:888] [0mPrepare costs: 654.015991 ms.
0, -51 -1.599340 
1, -52 -1.630699 
2, -52 -1.630699 
3, 61 1.912936 
4, 0 0.000000 
5, 0 0.000000 
6, 0 0.000000 
7, 0 0.000000 
8, 0 0.000000 
9, 0 0.000000 
10, 0 0.000000 
11, 0 0.000000 
12, 0 0.000000 
13, 0 0.000000 
14, 0 0.000000 
15, 0 0.000000 
16, 0 0.000000 
17, 0 0.000000 
18, 0 0.000000 
19, 0 0.000000 
20, 0 0.000000 
21, 0 0.000000 
22, 0 0.000000 
23, 0 0.000000 
24, 0 0.000000 
25, 0 0.000000 
26, 0 0.000000 
27, 0 0.000000 
28, 0 0.000000 
29, 0 0.000000 
30, 0 0.000000 
31, 0 0.000000 
32, 0 0.000000 
33, 0 0.000000 
34, 0 0.000000 
35, 0 0.000000 
36, 0 0.000000 
37, 0 0.000000 
38, 0 0.000000 
39, 0 0.000000 
40, 0 0.000000 
41, 0 0.000000 
42, 0 0.000000 
43, 0 0.000000 
44, 0 0.000000 
45, 0 0.000000 
46, 0 0.000000 
47, 0 0.000000 
48, 0 0.000000 
49, 0 0.000000 
50, 0 0.000000 
51, 0 0.000000 
52, 0 0.000000 
53, 0 0.000000 
54, 0 0.000000 
55, 0 0.000000 
56, 0 0.000000 
57, 0 0.000000 
58, 0 0.000000 
59, 0 0.000000 
60, 0 0.000000 
61, 0 0.000000 
62, 0 0.000000 
63, 0 0.000000 
64, 0 0.000000 
65, 0 0.000000 
66, 0 0.000000 
67, 0 0.000000 
68, 0 0.000000 
69, 0 0.000000 
70, 0 0.000000 
71, 0 0.000000 
72, 0 0.000000 
73, 0 0.000000 
74, 0 0.000000 
75, 0 0.000000 
76, 0 0.000000 
77, 0 0.000000 
78, 0 0.000000 
79, 0 0.000000 
80, 0 0.000000 
81, 0 0.000000 
82, 0 0.000000 
83, 0 0.000000 
84, 0 0.000000 
85, 0 0.000000 
86, 0 0.000000 
87, 0 0.000000 
88, 0 0.000000 
89, 0 0.000000 
90, 0 0.000000 
91, 0 0.000000 
92, 0 0.000000 
93, 0 0.000000 
94, 0 0.000000 
95, 0 0.000000 
96, 0 0.000000 
97, 0 0.000000 
98, 0 0.000000 
99, 0 0.000000 
[0;32m[INFO][2021-11-30 11:07:01.423][pplnn.cc:681] [0m----- input info -----
[0;32m[INFO][2021-11-30 11:07:01.423][pplnn.cc:684] [0minput[0]:
[0;32m[INFO][2021-11-30 11:07:01.423][pplnn.cc:685] [0m    name: 140
[0;32m[INFO][2021-11-30 11:07:01.423][pplnn.cc:692] [0m    dim(s): 1 64 56 56
[0;32m[INFO][2021-11-30 11:07:01.423][pplnn.cc:694] [0m    DataType: FLOAT32
[0;32m[INFO][2021-11-30 11:07:01.423][pplnn.cc:695] [0m    DataFormat: NDARRAY
[0;32m[INFO][2021-11-30 11:07:01.423][pplnn.cc:696] [0m    NumBytesIncludePadding: 802816
[0;32m[INFO][2021-11-30 11:07:01.423][pplnn.cc:697] [0m    NumBytesExcludePadding: 802816
[0;32m[INFO][2021-11-30 11:07:01.423][pplnn.cc:700] [0m----- output info -----
[0;32m[INFO][2021-11-30 11:07:01.423][pplnn.cc:703] [0moutput[0]:
[0;32m[INFO][2021-11-30 11:07:01.423][pplnn.cc:704] [0m    name: 146
[0;32m[INFO][2021-11-30 11:07:01.423][pplnn.cc:711] [0m    dim(s): 1 128 28 28
[0;32m[INFO][2021-11-30 11:07:01.423][pplnn.cc:713] [0m    DataType: FLOAT32
[0;32m[INFO][2021-11-30 11:07:01.423][pplnn.cc:714] [0m    DataFormat: NDARRAY
[0;32m[INFO][2021-11-30 11:07:01.423][pplnn.cc:715] [0m    NumBytesIncludePadding: 401408
[0;32m[INFO][2021-11-30 11:07:01.423][pplnn.cc:716] [0m    NumBytesExcludePadding: 401408
[0;32m[INFO][2021-11-30 11:07:01.423][pplnn.cc:719] [0m----------------------
[0;32m[INFO][2021-11-30 11:07:01.423][pplnn.cc:904] [0mRun() costs: 21.191999 ms.
[0;32m[INFO][2021-11-30 11:07:01.427][pplnn.cc:912] [0mRun ok
==29138== Profiling application: ./pplnn-build/tools/pplnn --use-cuda --onnx-model /mnt/hpc/share/xusi/QuantRes18Splitter/Conv_23/model.onnx --inputs /mnt/hpc/share/xusi/QuantRes18Splitter/LayerOutput/140_decorated_by_act_quant.bin --quantization /mnt/hpc/share/xusi/QuantRes18Splitter/re
==29138== Profiling result:
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
 GPU activities:   48.17%  356.64us         2  178.32us  12.768us  343.87us  void cuda_kernel_cvtformat<char, CVTFormatMode=31>(char*, char*, ReFormatParam)
                   23.81%  176.29us         5  35.257us  10.688us  64.992us  [CUDA memcpy DtoH]
                   14.48%  107.20us        20  5.3600us  1.3120us  70.464us  [CUDA memcpy HtoD]
                    4.06%  30.048us         3  10.016us  8.3840us  13.248us  void cuda_kernel_cvtformat<float, CVTFormatMode=32>(float*, float*, ReFormatParam)
                    3.65%  27.040us         3  9.0130us  7.2960us  12.256us  void cuda_kernel_cvt<CVTTypeMode=1>(unsigned long, int, int, void const *, ReFormatParam, void*)
                    2.90%  21.472us         2  10.736us  3.7120us  17.760us  void cuda_kernel_cvt<CVTTypeMode=2>(unsigned long, int, int, void const *, ReFormatParam, void*)
                    2.30%  17.056us         4  4.2640us  2.3040us  7.4880us  [CUDA memcpy DtoD]
                    0.44%  3.2320us         1  3.2320us  3.2320us  3.2320us  void group_padding<float>(float*, float*, unsigned long, int, int, int, int)
                    0.19%  1.3760us         1  1.3760us  1.3760us  1.3760us  [CUDA memset]
      API calls:   96.09%  262.28ms         2  131.14ms  13.287us  262.26ms  cudaStreamCreate
                    1.73%  4.7167ms         1  4.7167ms  4.7167ms  4.7167ms  cuModuleLoadDataEx
                    0.50%  1.3605ms        27  50.388us  2.9230us  395.71us  cudaMemcpyAsync
                    0.49%  1.3277ms        20  66.383us  3.6350us  334.44us  cudaFree
                    0.42%  1.1346ms        20  56.732us  1.3240us  387.43us  cudaMalloc
                    0.28%  758.98us       101  7.5140us     143ns  342.89us  cuDeviceGetAttribute
                    0.24%  654.33us         1  654.33us  654.33us  654.33us  cuDeviceTotalMem
                    0.10%  276.43us         1  276.43us  276.43us  276.43us  cuModuleUnload
                    0.05%  144.55us        11  13.140us  6.9920us  24.040us  cudaLaunchKernel
                    0.04%  120.60us         1  120.60us  120.60us  120.60us  cuDeviceGetName
                    0.01%  36.805us         2  18.402us  16.629us  20.176us  cudaMemcpy
                    0.01%  21.781us         2  10.890us  5.7450us  16.036us  cudaStreamDestroy
                    0.01%  18.539us         5  3.7070us  2.9100us  4.6890us  cudaStreamSynchronize
                    0.01%  15.950us         1  15.950us  15.950us  15.950us  cudaDeviceSynchronize
                    0.01%  15.471us         3  5.1570us  4.0410us  6.0700us  cudaEventRecord
                    0.00%  12.488us         1  12.488us  12.488us  12.488us  cudaMemset
                    0.00%  9.7510us         2  4.8750us  1.8470us  7.9040us  cudaSetDevice
                    0.00%  8.5490us         3  2.8490us     444ns  7.6300us  cudaEventCreateWithFlags
                    0.00%  8.5210us         1  8.5210us  8.5210us  8.5210us  cuDeviceGetPCIBusId
                    0.00%  6.5900us         3  2.1960us     702ns  4.8720us  cudaEventDestroy
                    0.00%  4.9370us         1  4.9370us  4.9370us  4.9370us  cudaEventSynchronize
                    0.00%  3.2330us         2  1.6160us     410ns  2.8230us  cudaDeviceGetAttribute
                    0.00%  2.3290us         2  1.1640us     169ns  2.1600us  cuDeviceGet
                    0.00%  1.5950us         3     531ns     177ns  1.1780us  cuDeviceGetCount
                    0.00%  1.2790us         1  1.2790us  1.2790us  1.2790us  cuModuleGetFunction
                    0.00%     260ns         1     260ns     260ns     260ns  cuDeviceGetUuid
